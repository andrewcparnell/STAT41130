{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# STAT41130 — Transformers for Weather Forecasting (PyTorch + MetPy)\n\nThis notebook mirrors the lecture structure:\n1. **Tiny Transformer on fake data** — the simplest possible settings.\n2. **Single-site weather variable** — slightly richer Transformer fitted to a real series from MetPy.\n3. **Multivariate, multi-head attention** — a more complex model using multiple weather variables from the same site.\n\n> Notes\n> * The MetPy sections use a small NARR sample dataset that downloads automatically on first use.\n> * All models are intentionally small and trained briefly to keep runtime short.\n> * You can scale up `d_model`, `num_heads`, `num_layers`, and epochs for better skill."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# If needed, install dependencies (uncomment in a fresh environment)\n# %pip install torch --quiet\n# %pip install metpy xarray netCDF4 numpy pandas matplotlib scikit-learn --quiet"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "import math\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\ntorch.manual_seed(0)\nnp.random.seed(0)\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nDEVICE"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def make_sequences(array_like, seq_len, target_horizon=1, target_cols=None):\n    \"\"\"\n    Turn a 2D array-like time series (T, F) into supervised sequences:\n    X: (N, seq_len, F_in), y: (N, F_out), predicting target at t+target_horizon.\n    If target_cols is None, default to the first column.\n    \"\"\"\n    data = np.asarray(array_like)\n    T, F = data.shape\n    if target_cols is None:\n        target_cols = [0]\n    X, y = [], []\n    for t in range(T - seq_len - target_horizon + 1):\n        X.append(data[t:t+seq_len, :])\n        y.append(data[t+seq_len+target_horizon-1, target_cols])\n    return np.array(X, dtype=np.float32), np.array(y, dtype=np.float32)\n\nclass SeqDataset(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.float32)\n    def __len__(self):\n        return self.X.shape[0]\n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx]\n\ndef train_epoch(model, loader, criterion, optimizer):\n    model.train()\n    total = 0.0\n    for Xb, yb in loader:\n        Xb, yb = Xb.to(DEVICE), yb.to(DEVICE)\n        optimizer.zero_grad()\n        out = model(Xb)\n        loss = criterion(out, yb)\n        loss.backward()\n        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        total += loss.item() * Xb.size(0)\n    return total / len(loader.dataset)\n\n@torch.no_grad()\ndef eval_epoch(model, loader, criterion):\n    model.eval()\n    total = 0.0\n    for Xb, yb in loader:\n        Xb, yb = Xb.to(DEVICE), yb.to(DEVICE)\n        out = model(Xb)\n        loss = criterion(out, yb)\n        total += loss.item() * Xb.size(0)\n    return total / len(loader.dataset)\n\n@torch.no_grad()\ndef predict(model, X):\n    model.eval()\n    X = torch.tensor(X, dtype=torch.float32, device=DEVICE)\n    return model(X).cpu().numpy()\n\ndef plot_series(true_series, pred_series=None, title=\"\"):\n    plt.figure(figsize=(8,3))\n    plt.plot(true_series, label=\"true\")\n    if pred_series is not None:\n        plt.plot(pred_series, label=\"pred\")\n    plt.title(title)\n    plt.legend()\n    plt.show()"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "class PositionalEncoding(nn.Module):\n    def __init__(self, d_model: int, max_len: int = 1000):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        self.register_buffer('pe', pe.unsqueeze(0))  # (1, max_len, d_model)\n    def forward(self, x):\n        # x: (B, T, d_model)\n        T = x.size(1)\n        return x + self.pe[:, :T, :]\n\ndef generate_causal_mask(T: int, device=None):\n    # Upper-triangular mask with -inf above diagonal to prevent attention to future positions\n    mask = torch.triu(torch.ones(T, T), diagonal=1)\n    mask = mask.masked_fill(mask==1, float('-inf'))\n    return mask.to(device) if device is not None else mask"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "class TransformerForecaster(nn.Module):\n    def __init__(self, input_size, d_model=32, nhead=1, num_layers=1, dim_ff=64, dropout=0.0, output_size=1):\n        super().__init__()\n        self.input_proj = nn.Linear(input_size, d_model)\n        self.posenc = PositionalEncoding(d_model)\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=d_model, nhead=nhead, dim_feedforward=dim_ff,\n            dropout=dropout, batch_first=True, activation='gelu')\n        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n        self.readout = nn.Linear(d_model, output_size)\n    def forward(self, x):\n        # x: (B, T, F)\n        z = self.input_proj(x)            # (B, T, d_model)\n        z = self.posenc(z)                # add positional encoding\n        T = z.size(1)\n        mask = generate_causal_mask(T, device=z.device)  # causal mask\n        z = self.encoder(z, mask=mask)   # (B, T, d_model)\n        last = z[:, -1, :]               # use final token representation\n        yhat = self.readout(last)        # (B, output_size)\n        return yhat"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1) Tiny Transformer on Fake Data\n\nTask: Predict the next value of a noisy sinusoid from a short look-back window.  \nWe use the **smallest** viable Transformer: `d_model=16`, `nhead=1`, `num_layers=1`."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Synthetic data\nT = 500\nt = np.arange(T)\nseries = np.sin(0.03 * t) + 0.4*np.sin(0.07 * t + 0.7) + 0.1*np.random.randn(T)\ndata = series.reshape(-1, 1)\n\nseq_len = 24\nX, y = make_sequences(data, seq_len=seq_len, target_horizon=1, target_cols=[0])\n\nX_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size=0.25, shuffle=False)\n\n# Simple scaling of inputs (targets stay unscaled)\nscaler = StandardScaler().fit(X_tr.reshape(-1, X_tr.shape[-1]))\nX_tr_sc = scaler.transform(X_tr.reshape(-1, X_tr.shape[-1])).reshape(X_tr.shape)\nX_val_sc = scaler.transform(X_val.reshape(-1, X_val.shape[-1])).reshape(X_val.shape)\n\ntrain_ds = SeqDataset(X_tr_sc, y_tr)\nval_ds   = SeqDataset(X_val_sc, y_val)\ntrain_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\nval_loader   = DataLoader(val_ds,   batch_size=64)\n\nmodel_tiny = TransformerForecaster(input_size=1, d_model=16, nhead=1, num_layers=1, dim_ff=32, dropout=0.0, output_size=1).to(DEVICE)\nopt = torch.optim.Adam(model_tiny.parameters(), lr=1e-3)\ncrit = nn.MSELoss()\n\nEPOCHS = 25\nhist_tiny = {\"train\": [], \"val\": []}\nfor ep in range(EPOCHS):\n    tr = train_epoch(model_tiny, train_loader, crit, opt)\n    va = eval_epoch(model_tiny, val_loader, crit)\n    hist_tiny[\"train\"].append(tr)\n    hist_tiny[\"val\"].append(va)\n\nprint(f\"Tiny Transformer — Final MSE: Train={hist_tiny['train'][-1]:.4f}, Val={hist_tiny['val'][-1]:.4f}\")"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Visualize validation predictions\ny_pred_val = predict(model_tiny, X_val_sc)\nplot_series(y_val.squeeze(), y_pred_val.squeeze(), title=\"Tiny Transformer — Next-step Forecast (Validation)\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2) Single-Site Weather Variable (MetPy) — Slightly Larger Transformer\n\nWe extract a **temperature** series at one grid point from MetPy's sample NARR file,\nbuild sliding windows, and fit a small Transformer with `d_model=32`, `nhead=2`, `num_layers=1`."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "import xarray as xr\nfrom metpy.cbook import get_test_data\nfrom metpy.units import units\n\n# Load NARR sample file (downloads if missing)\npath = get_test_data('narr_example.nc', as_file_obj=False)\nds = xr.open_dataset(path).metpy.parse_cf()\n\n# Pick temperature (isobaric) and a grid point\nvarname = \"Temperature_isobaric\"\nassert varname in ds.variables, \"Expected 'Temperature_isobaric' not found. Explore ds to select another variable.\"\ntemp = ds[varname]  # (time, isobaric, y, x)\n\n# Choose one level and grid point\ntemp_point = temp.isel(isobaric=0, y=20, x=20).metpy.convert_units('degC')\nseries_w = temp_point.values.astype(np.float32).reshape(-1, 1)\n\nseq_len = 12\nXw, yw = make_sequences(series_w, seq_len=seq_len, target_horizon=1, target_cols=[0])\n\nXw_tr, Xw_val, yw_tr, yw_val = train_test_split(Xw, yw, test_size=0.25, shuffle=False)\n\n# input scaling\nscaler_w = StandardScaler().fit(Xw_tr.reshape(-1, Xw_tr.shape[-1]))\nXw_tr_sc = scaler_w.transform(Xw_tr.reshape(-1, Xw_tr.shape[-1])).reshape(Xw_tr.shape)\nXw_val_sc= scaler_w.transform(Xw_val.reshape(-1, Xw_val.shape[-1])).reshape(Xw_val.shape)\n\ntrain_ds_w = SeqDataset(Xw_tr_sc, yw_tr)\nval_ds_w   = SeqDataset(Xw_val_sc,  yw_val)\ntrain_loader_w = DataLoader(train_ds_w, batch_size=64, shuffle=True)\nval_loader_w   = DataLoader(val_ds_w,   batch_size=64)\n\nmodel_w = TransformerForecaster(input_size=1, d_model=32, nhead=2, num_layers=1, dim_ff=64, dropout=0.1, output_size=1).to(DEVICE)\nopt_w = torch.optim.Adam(model_w.parameters(), lr=1e-3)\ncrit_w = nn.MSELoss()\n\nEPOCHS = 20\nhist_w = {\"train\": [], \"val\": []}\nfor ep in range(EPOCHS):\n    tr = train_epoch(model_w, train_loader_w, crit_w, opt_w)\n    va = eval_epoch(model_w, val_loader_w, crit_w)\n    hist_w[\"train\"].append(tr)\n    hist_w[\"val\"].append(va)\n\nprint(f\"Single-var Transformer — Final MSE: Train={hist_w['train'][-1]:.4f}, Val={hist_w['val'][-1]:.4f}\")"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Visualize validation predictions on MetPy temperature\ny_pred_w = predict(model_w, Xw_val_sc)\nplot_series(yw_val.squeeze(), y_pred_w.squeeze(), title=\"MetPy Temperature — Transformer Validation\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3) Multivariate, Multi-Head Transformer on MetPy\n\nWe now include **multiple variables** from the same site (grid point), e.g.:\n- Temperature (degC) at a chosen isobaric level\n- Geopotential height (gpm) at the same level (if available)\n- U/V wind components at the same level (if available)\n\nInputs are multi-feature sequences; the model uses **multi-head attention** (`nhead=4`) and a deeper encoder.\nThe target remains the **next-step temperature** for clarity."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Collect multiple variables if present; gracefully fall back if missing\nfeatures = []\n\n# Temperature (must exist)\ntemp0 = ds['Temperature_isobaric'].isel(isobaric=0, y=20, x=20).metpy.convert_units('degC').values.astype(np.float32)\nfeatures.append(temp0)\n\n# Try geopotential height\nif 'Geopotential_height_isobaric' in ds.variables:\n    hgt0 = ds['Geopotential_height_isobaric'].isel(isobaric=0, y=20, x=20).values.astype(np.float32)\n    features.append(hgt0)\n# Try u/v wind\nif 'u-component_of_wind_isobaric' in ds.variables:\n    u0 = ds['u-component_of_wind_isobaric'].isel(isobaric=0, y=20, x=20).values.astype(np.float32)\n    features.append(u0)\nif 'v-component_of_wind_isobaric' in ds.variables:\n    v0 = ds['v-component_of_wind_isobaric'].isel(isobaric=0, y=20, x=20).values.astype(np.float32)\n    features.append(v0)\n\n# Stack features: shape (T, F)\nmulti_X = np.stack(features, axis=1)\ntarget_cols = [0]  # predict temperature next step\n\nseq_len_m = 12\nXm, ym = make_sequences(multi_X, seq_len=seq_len_m, target_horizon=1, target_cols=target_cols)\n\nXm_tr, Xm_val, ym_tr, ym_val = train_test_split(Xm, ym, test_size=0.25, shuffle=False)\n\n# scale inputs only\nscaler_m = StandardScaler().fit(Xm_tr.reshape(-1, Xm_tr.shape[-1]))\nXm_tr_sc = scaler_m.transform(Xm_tr.reshape(-1, Xm_tr.shape[-1])).reshape(Xm_tr.shape)\nXm_val_sc= scaler_m.transform(Xm_val.reshape(-1, Xm_val.shape[-1])).reshape(Xm_val.shape)\n\ntrain_ds_m = SeqDataset(Xm_tr_sc, ym_tr)\nval_ds_m   = SeqDataset(Xm_val_sc,  ym_val)\ntrain_loader_m = DataLoader(train_ds_m, batch_size=64, shuffle=True)\nval_loader_m   = DataLoader(val_ds_m,   batch_size=64)\n\ninput_size_m = Xm_tr_sc.shape[2]\n\n# Multi-head, deeper encoder\nmodel_m = TransformerForecaster(\n    input_size=input_size_m,\n    d_model=64,\n    nhead=4,\n    num_layers=2,\n    dim_ff=128,\n    dropout=0.1,\n    output_size=1\n).to(DEVICE)\n\nopt_m = torch.optim.Adam(model_m.parameters(), lr=1e-3)\ncrit_m = nn.MSELoss()\n\nEPOCHS = 25\nhist_m = {\"train\": [], \"val\": []}\nfor ep in range(EPOCHS):\n    tr = train_epoch(model_m, train_loader_m, crit_m, opt_m)\n    va = eval_epoch(model_m, val_loader_m, crit_m)\n    hist_m[\"train\"].append(tr)\n    hist_m[\"val\"].append(va)\n\nprint(f\"Multivariate Transformer — Final MSE: Train={hist_m['train'][-1]:.4f}, Val={hist_m['val'][-1]:.4f}\")"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Plot predictions vs truth\ny_pred_m = predict(model_m, Xm_val_sc)\nplot_series(ym_val.squeeze(), y_pred_m.squeeze(), title=\"Multivariate Transformer (nhead=4) — Validation\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Wrap-up & Ideas to Extend\n\n- Try longer windows, multistep horizons (predict t+1..t+H), or multi-target outputs.\n- Add exogenous variables (e.g., pressure, humidity) and calendar/diurnal encodings.\n- Use learning rate schedulers, early stopping, and weight decay.\n- Scale up model capacity or train longer on GPU for improved accuracy."
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.8"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 5
}