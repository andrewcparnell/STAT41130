{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# STAT41130 \u2014 PyTorch by Example (Weather & Climate Edition)\n",
        "*Auto-generated on 2025-09-25*\n",
        "\n",
        "This notebook is a hands-on introduction to **PyTorch** using **real weather & climate datasets**.  \n",
        "We start with tiny fully-connected neural nets (MLPs) to see training dynamics and convergence, then move to **1D and 2D Convolutional Neural Networks (CNNs)** with lightweight meteorological data.\n",
        "\n",
        "**Datasets used (small and fetchable):**\n",
        "- **Daily Minimum Temperatures (Sydney)** \u2014 a classic weather time series (CSV, small).  \n",
        "- **Mauna Loa Atmospheric CO\u2082 (monthly means)** \u2014 canonical climate series (CSV/text, tiny).  \n",
        "- **NARR reanalysis example** via **MetPy** test data \u2014 small NetCDF field we can treat like an image.\n",
        "\n",
        "> Tip: GPU is nice but CPU is fine \u2014 all examples are intentionally small."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0) Setup\n",
        "\n",
        "We install/import a minimal scientific stack. If you already have these installed, the install cells will quickly no-op."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# If running in a fresh environment, uncomment the following lines.\n",
        "# %pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
        "# %pip install -q numpy pandas matplotlib scikit-learn xarray netCDF4 metpy requests tqdm\n",
        "\n",
        "import math, os, io, sys, json, time, textwrap, random, pathlib, itertools\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Optional: xarray + MetPy for the 2D meteorological field\n",
        "import xarray as xr\n",
        "try:\n",
        "    from metpy.cbook import get_test_data\n",
        "    import metpy  # noqa: F401\n",
        "    METPY_OK = True\n",
        "except Exception as e:\n",
        "    METPY_OK = False\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED); np.random.seed(SEED); random.seed(SEED)\n",
        "DEVICE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Helper plotting utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def plot_series(y, yhat=None, title=\"\", start=None, end=None):\n",
        "    # Plot a time series and optional predictions.\n",
        "    plt.figure(figsize=(10,3))\n",
        "    if start is None: start = 0\n",
        "    if end is None: end = len(y)\n",
        "    plt.plot(range(start, end), y[start:end], label=\"observed\")\n",
        "    if yhat is not None:\n",
        "        plt.plot(range(start, start+len(yhat)), yhat, label=\"pred\", linestyle=\"--\")\n",
        "    plt.title(title); plt.xlabel(\"t\"); plt.ylabel(\"value\"); plt.legend(); plt.tight_layout(); plt.show()\n",
        "\n",
        "\n",
        "def training_curve(train_losses, val_losses=None, title=\"Training curve\"):\n",
        "    plt.figure(figsize=(6,3))\n",
        "    plt.plot(train_losses, label=\"train\")\n",
        "    if val_losses is not None:\n",
        "        plt.plot(val_losses, label=\"val\")\n",
        "    plt.xlabel(\"epoch\"); plt.ylabel(\"loss\"); plt.title(title); plt.legend(); plt.tight_layout(); plt.show()\n",
        "\n",
        "\n",
        "def show_kernels_2d(weight_tensor, max_k=8):\n",
        "    # Visualise learned 2D conv kernels (out_channels x in_channels x kH x kW).\n",
        "    W = weight_tensor.detach().cpu().numpy()\n",
        "    oc, ic, kh, kw = W.shape\n",
        "    n = min(oc, max_k)\n",
        "    cols = n\n",
        "    plt.figure(figsize=(1.8*cols, 1.8))\n",
        "    for i in range(n):\n",
        "        plt.subplot(1, cols, i+1)\n",
        "        plt.imshow(W[i,0], aspect='auto')\n",
        "        plt.axis('off')\n",
        "        plt.title(f\"k{i}\")\n",
        "    plt.tight_layout(); plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 1) Warm-up: Tensors, autograd, and a tiny linear regression\n",
        "\n",
        "We begin with the most basic concepts: tensors, automatic differentiation, and a minimal gradient descent loop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Synthetic linear data: y = 2.5 * x - 1.0 + noise\n",
        "n = 200\n",
        "x = torch.linspace(-2, 2, n).unsqueeze(1)\n",
        "true_w, true_b = 2.5, -1.0\n",
        "y = true_w * x + true_b + 0.2 * torch.randn_like(x)\n",
        "\n",
        "# Simple linear model\n",
        "model = nn.Linear(1, 1)\n",
        "opt = torch.optim.SGD(model.parameters(), lr=0.1)\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "train_losses = []\n",
        "for epoch in range(200):\n",
        "    opt.zero_grad()\n",
        "    yhat = model(x)\n",
        "    loss = loss_fn(yhat, y)\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    train_losses.append(loss.item())\n",
        "\n",
        "print(\"Learned params:\", [p.data.item() for p in model.parameters()])\n",
        "training_curve(train_losses, title=\"Linear regression: MSE vs epoch\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Takeaway:** PyTorch tracks operations on tensors, computes gradients via `.backward()`, and updates parameters with an optimizer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 2) Real data #1 \u2014 Daily Minimum Temperatures (Sydney)\n",
        "\n",
        "**Task:** Given the previous 7 days, predict the next day's minimum temperature.\n",
        "\n",
        "This is a small, classic weather time series. We\u2019ll frame it as supervised learning with sliding windows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import pandas as pd, requests\n",
        "\n",
        "URL_TEMP = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/daily-min-temperatures.csv\"\n",
        "df = pd.read_csv(URL_TEMP, parse_dates=[\"Date\"])\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Build supervised windows: X[t] = temps[t-7:t], y[t] = temp[t]\n",
        "values = df[\"Temp\"].values.astype(np.float32)\n",
        "window = 7\n",
        "X, Y = [], []\n",
        "for t in range(window, len(values)):\n",
        "    X.append(values[t-window:t])\n",
        "    Y.append(values[t])\n",
        "X = np.stack(X)   # (N, 7)\n",
        "Y = np.array(Y)   # (N, )\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, shuffle=False)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_s = scaler.fit_transform(X_train)\n",
        "X_test_s  = scaler.transform(X_test)\n",
        "\n",
        "X_train_t = torch.tensor(X_train_s, dtype=torch.float32).to(DEVICE)\n",
        "y_train_t = torch.tensor(y_train,  dtype=torch.float32).unsqueeze(1).to(DEVICE)\n",
        "X_test_t  = torch.tensor(X_test_s, dtype=torch.float32).to(DEVICE)\n",
        "y_test_t  = torch.tensor(y_test,   dtype=torch.float32).unsqueeze(1).to(DEVICE)\n",
        "\n",
        "len(X_train_t), len(X_test_t), X_train_t.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 A tiny MLP (fully-connected network)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 16),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(16, 1),\n",
        "        )\n",
        "    def forward(self, x): return self.net(x)\n",
        "\n",
        "mlp = MLP(in_dim=7).to(DEVICE)\n",
        "opt = torch.optim.Adam(mlp.parameters(), lr=1e-3)\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "train_losses, val_losses = [], []\n",
        "X_tr, X_val, y_tr, y_val = X_train_t[:-200], X_train_t[-200:], y_train_t[:-200], y_train_t[-200:]\n",
        "for epoch in range(300):\n",
        "    mlp.train()\n",
        "    opt.zero_grad()\n",
        "    yhat = mlp(X_tr)\n",
        "    loss = loss_fn(yhat, y_tr)\n",
        "    loss.backward(); opt.step()\n",
        "    train_losses.append(loss.item())\n",
        "\n",
        "    mlp.eval()\n",
        "    with torch.no_grad():\n",
        "        val = loss_fn(mlp(X_val), y_val).item()\n",
        "        val_losses.append(val)\n",
        "\n",
        "print(\"Final val MSE:\", val_losses[-1])\n",
        "training_curve(train_losses, val_losses, title=\"MLP on daily min temps\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Evaluate on held-out test\n",
        "mlp.eval()\n",
        "with torch.no_grad():\n",
        "    pred = mlp(X_test_t).cpu().numpy().squeeze()\n",
        "mse = mean_squared_error(y_test, pred)\n",
        "r2 = r2_score(y_test, pred)\n",
        "print(\"Test MSE:\", mse, \"R^2:\", r2)\n",
        "\n",
        "plot_series(df[\"Temp\"].values[-len(y_test):], yhat=pred, title=\"Test window: observed vs MLP prediction\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Dropout and BatchNorm demo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "class MLP_DropBN(nn.Module):\n",
        "    def __init__(self, in_dim):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(in_dim, 32)\n",
        "        self.bn1 = nn.BatchNorm1d(32)\n",
        "        self.fc2 = nn.Linear(32, 16)\n",
        "        self.bn2 = nn.BatchNorm1d(16)\n",
        "        self.fc3 = nn.Linear(16, 1)\n",
        "        self.drop = nn.Dropout(p=0.2)\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.fc1(x)))\n",
        "        x = self.drop(x)\n",
        "        x = F.relu(self.bn2(self.fc2(x)))\n",
        "        x = self.drop(x)\n",
        "        return self.fc3(x)\n",
        "\n",
        "mlp2 = MLP_DropBN(7).to(DEVICE)\n",
        "opt = torch.optim.Adam(mlp2.parameters(), lr=1e-3)\n",
        "loss_fn = nn.MSELoss()\n",
        "train_losses = []\n",
        "for epoch in range(300):\n",
        "    mlp2.train(); opt.zero_grad()\n",
        "    loss = loss_fn(mlp2(X_train_t), y_train_t)\n",
        "    loss.backward(); opt.step()\n",
        "    train_losses.append(loss.item())\n",
        "\n",
        "mlp2.eval()\n",
        "with torch.no_grad():\n",
        "    pred2 = mlp2(X_test_t).cpu().numpy().squeeze()\n",
        "print(\"Test MSE:\", mean_squared_error(y_test, pred2))\n",
        "training_curve(train_losses, title=\"MLP with Dropout + BatchNorm (train loss)\")\n",
        "plot_series(df[\"Temp\"].values[-len(y_test):], yhat=pred2, title=\"Test: Dropout+BN MLP\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 3) Real data #2 \u2014 Mauna Loa CO\u2082 (monthly) with 1D CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Fetch NOAA Mauna Loa monthly CO2 data (CSV or TXT)\n",
        "import pandas as pd, io, requests, numpy as np\n",
        "\n",
        "def load_maunaloa_monthly():\n",
        "    urls = [\n",
        "        \"https://gml.noaa.gov/webdata/ccgg/trends/co2/co2_mm_gl.csv\",\n",
        "        \"https://gml.noaa.gov/webdata/ccgg/trends/co2/co2_mm_mlo.csv\",\n",
        "        \"https://gml.noaa.gov/webdata/ccgg/trends/co2/co2_mm_gl.txt\",\n",
        "    ]\n",
        "    for u in urls:\n",
        "        try:\n",
        "            r = requests.get(u, timeout=20)\n",
        "            if r.ok:\n",
        "                if u.endswith(\".csv\"):\n",
        "                    df = pd.read_csv(io.StringIO(r.text), comment=\"#\")\n",
        "                    candidates = [c for c in df.columns if c.strip().lower() in (\"average\",\"interpolated\",\"trend\")]\n",
        "                    if not candidates:\n",
        "                        nums = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "                        candidates = [nums[0]] if nums else []\n",
        "                    df = df.rename(columns={candidates[0]: \"co2\"})\n",
        "                    df = df.dropna(subset=[\"co2\"]).reset_index(drop=True)\n",
        "                    return df\n",
        "                else:\n",
        "                    lines = [ln for ln in r.text.splitlines() if not ln.strip().startswith(\"#\")]\n",
        "                    data = []\n",
        "                    for ln in lines:\n",
        "                        parts = ln.split(\",\")\n",
        "                        if len(parts) >= 5:\n",
        "                            try:\n",
        "                                yr = int(parts[0]); mo = int(parts[1]); val = float(parts[4])\n",
        "                                data.append((f\"{yr}-{mo:02d}\", val))\n",
        "                            except:\n",
        "                                pass\n",
        "                    df = pd.DataFrame(data, columns=[\"date\", \"co2\"])\n",
        "                    return df\n",
        "        except Exception:\n",
        "            continue\n",
        "    raise RuntimeError(\"Failed to download CO2 series\")\n",
        "\n",
        "co2 = load_maunaloa_monthly()\n",
        "co2.head()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Build 24-month windows for 1D CNN\n",
        "vals = co2[\"co2\"].values.astype(np.float32)\n",
        "context = 24\n",
        "X, Y = [], []\n",
        "for t in range(context, len(vals)):\n",
        "    X.append(vals[t-context:t])\n",
        "    Y.append(vals[t])\n",
        "X = np.array(X)[:, None, :]   # (N, C=1, L=context)\n",
        "Y = np.array(Y)[:, None]      # (N, 1)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(X, Y, test_size=0.2, shuffle=False)\n",
        "X_tr_t = torch.tensor(X_tr, dtype=torch.float32).to(DEVICE)\n",
        "y_tr_t = torch.tensor(y_tr, dtype=torch.float32).to(DEVICE)\n",
        "X_te_t = torch.tensor(X_te, dtype=torch.float32).to(DEVICE)\n",
        "y_te_t = torch.tensor(y_te, dtype=torch.float32).to(DEVICE)\n",
        "\n",
        "class TinyCNN1D(nn.Module):\n",
        "    def __init__(self, in_ch=1, width=16, k=3):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv1d(in_ch, width, kernel_size=k, padding=\"same\")\n",
        "        self.conv2 = nn.Conv1d(width, width, kernel_size=k, padding=\"same\")\n",
        "        self.head  = nn.Sequential(nn.AdaptiveAvgPool1d(1), nn.Flatten(), nn.Linear(width, 1))\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        return self.head(x)\n",
        "\n",
        "model = TinyCNN1D().to(DEVICE)\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=3e-3)\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "train_losses = []\n",
        "for epoch in range(400):\n",
        "    model.train(); opt.zero_grad()\n",
        "    yhat = model(X_tr_t)\n",
        "    loss = loss_fn(yhat, y_tr_t)\n",
        "    loss.backward(); opt.step()\n",
        "    train_losses.append(loss.item())\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    pred = model(X_te_t).cpu().numpy().squeeze()\n",
        "mse = mean_squared_error(y_te.squeeze(), pred)\n",
        "r2  = r2_score(y_te.squeeze(), pred)\n",
        "print(\"CO2 forecast \u2014 Test MSE:\", mse, \"R^2:\", r2)\n",
        "training_curve(train_losses, title=\"1D CNN on Mauna Loa CO\u2082\")\n",
        "plt.figure(figsize=(10,3))\n",
        "plt.plot(y_te.squeeze(), label=\"obs\")\n",
        "plt.plot(pred, label=\"pred\", linestyle=\"--\")\n",
        "plt.legend(); plt.title(\"Test period: CO\u2082 next-month prediction\"); plt.tight_layout(); plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 4) 2D CNN on a meteorological field (MetPy test data)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "assert METPY_OK, \"MetPy is required for the next section. Please install metpy and rerun the notebook.\"\n",
        "\n",
        "ds = xr.open_dataset(get_test_data('narr_example.nc', as_file_obj=False))\n",
        "# Select a single 2D field at one pressure level & time\n",
        "if \"Temperature\" in ds:\n",
        "    da = ds[\"Temperature\"].isel(time=0, isobaric=0)  # (y, x)\n",
        "else:\n",
        "    chosen = None\n",
        "    for name in ds.data_vars:\n",
        "        if ds[name].ndim >= 2:\n",
        "            chosen = ds[name]; break\n",
        "    if chosen.ndim == 3:\n",
        "        da = chosen.isel(time=0)\n",
        "        if da.ndim == 3 and \"isobaric\" in da.dims:\n",
        "            da = da.isel(isobaric=0)\n",
        "    else:\n",
        "        da = chosen\n",
        "\n",
        "field = np.array(da.values, dtype=np.float32)\n",
        "plt.figure(figsize=(5,4))\n",
        "plt.imshow(field, origin=\"lower\")\n",
        "plt.colorbar(); plt.title(\"Sample meteorological field\"); plt.tight_layout(); plt.show()\n",
        "\n",
        "H, W = field.shape\n",
        "mu = np.nanmean(field)\n",
        "labels = (field > mu).astype(np.int64)  # 1 if above-mean, else 0\n",
        "\n",
        "# Extract small patches for supervised classification\n",
        "patch = 9  # 9x9 neighborhood\n",
        "pad = patch//2\n",
        "field_nan = np.nan_to_num(field, nan=float(mu))\n",
        "Fpad = np.pad(field_nan, pad_width=pad, mode=\"reflect\")\n",
        "\n",
        "Xs, Ys = [], []\n",
        "for i in range(pad, pad+H):\n",
        "    for j in range(pad, pad+W):\n",
        "        win = Fpad[i-pad:i+pad+1, j-pad:j+pad+1]\n",
        "        Xs.append(win[None, ...])  # (1, patch, patch)\n",
        "        Ys.append(labels[i-pad, j-pad])\n",
        "Xs = np.stack(Xs).astype(np.float32)\n",
        "Ys = np.array(Ys).astype(np.int64)\n",
        "\n",
        "# Train/test split\n",
        "N = len(Ys)\n",
        "idx = np.arange(N); np.random.shuffle(idx)\n",
        "train_n = int(0.8*N)\n",
        "tr_idx, te_idx = idx[:train_n], idx[train_n:]\n",
        "X_tr, X_te = Xs[tr_idx], Xs[te_idx]\n",
        "y_tr, y_te = Ys[tr_idx], Ys[te_idx]\n",
        "\n",
        "X_tr_t = torch.tensor(X_tr, dtype=torch.float32).to(DEVICE)\n",
        "y_tr_t = torch.tensor(y_tr, dtype=torch.long).to(DEVICE)\n",
        "X_te_t = torch.tensor(X_te, dtype=torch.float32).to(DEVICE)\n",
        "y_te_t = torch.tensor(y_te, dtype=torch.long).to(DEVICE)\n",
        "\n",
        "class TinyCNN2D(nn.Module):\n",
        "    def __init__(self, in_ch=1, k=3, c=8):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, c, kernel_size=k, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(c, c, kernel_size=k, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(c, 2*c, kernel_size=k, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "        )\n",
        "        self.classifier = nn.Linear(2*c, 2)\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return self.classifier(x)\n",
        "\n",
        "net = TinyCNN2D().to(DEVICE)\n",
        "opt = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "batch = 256\n",
        "train_losses = []\n",
        "for epoch in range(10):  # quick\n",
        "    net.train()\n",
        "    perm = torch.randperm(X_tr_t.size(0))\n",
        "    for i in range(0, X_tr_t.size(0), batch):\n",
        "        ix = perm[i:i+batch]\n",
        "        xb, yb = X_tr_t[ix], y_tr_t[ix]\n",
        "        opt.zero_grad()\n",
        "        logits = net(xb)\n",
        "        loss = criterion(logits, yb)\n",
        "        loss.backward(); opt.step()\n",
        "        train_losses.append(loss.item())\n",
        "\n",
        "net.eval()\n",
        "with torch.no_grad():\n",
        "    logits = net(X_te_t)\n",
        "    pred = logits.argmax(1).cpu().numpy()\n",
        "    acc = (pred == y_te).mean()\n",
        "print(\"Patch classification accuracy:\", acc)\n",
        "training_curve(train_losses, title=\"2D CNN training loss\")\n",
        "show_kernels_2d(net.features[0].weight, max_k=8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 5) PyTorch training patterns: DataLoader, early stopping, schedulers, saving"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "class WindowDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n",
        "    def __len__(self): return len(self.X)\n",
        "    def __getitem__(self, i): return self.X[i], self.y[i]\n",
        "\n",
        "# Reuse temperature dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "ds = WindowDataset(X_train_s, y_train)\n",
        "dl = DataLoader(ds, batch_size=64, shuffle=True)\n",
        "\n",
        "model = MLP(7).to(DEVICE)\n",
        "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "best_loss = float(\"inf\"); patience, wait = 10, 0\n",
        "history = []\n",
        "for epoch in range(200):\n",
        "    model.train()\n",
        "    for xb, yb in dl:\n",
        "        xb = xb.to(DEVICE); yb = yb.to(DEVICE)\n",
        "        opt.zero_grad()\n",
        "        loss = loss_fn(model(xb), yb)\n",
        "        loss.backward(); opt.step()\n",
        "    # simple val check\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val = loss_fn(model(torch.tensor(X_test_s, dtype=torch.float32).to(DEVICE)),\n",
        "                      torch.tensor(y_test, dtype=torch.float32).unsqueeze(1).to(DEVICE)).item()\n",
        "    history.append(val)\n",
        "    if val < best_loss - 1e-4:\n",
        "        best_loss, wait = val, 0\n",
        "        torch.save(model.state_dict(), \"best_mlp.pt\")\n",
        "    else:\n",
        "        wait += 1\n",
        "    if wait >= patience:\n",
        "        print(\"Early stopping at epoch\", epoch)\n",
        "        break\n",
        "\n",
        "training_curve(history, title=\"Early stopping (validation loss)\")\n",
        "print(\"Best val:\", best_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 6) Next steps\n",
        "- Compare MLP vs 1D CNN on the same sequence.\n",
        "- Turn the 2D toy task into a real prediction with past-time inputs.\n",
        "- Try BatchNorm2d, Dropout2d, and weight decay for generalization."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}