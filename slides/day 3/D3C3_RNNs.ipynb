{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "866fa264",
   "metadata": {},
   "source": [
    "# Irish Weather — Simple RNN & LSTM (Dublin t2m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ccb641",
   "metadata": {},
   "source": [
    "This lightweight notebook mirrors the lecture slides and keeps code very small.\n",
    "We use **one CSV**: `era5_ireland3_t2m_wind_2024.csv` and predict the **next hour** of Dublin temperature from the **previous 24 hours**.\n",
    "Models: (1) `nn.RNN` (vanilla RNN), (2) `nn.LSTM` — no GRUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff23aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pandas numpy scikit-learn torch matplotlib\n",
    "\n",
    "import warnings, pathlib\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "CSV_PATH = \"era5_ireland3_t2m_wind_2024.csv\"\n",
    "assert pathlib.Path(CSV_PATH).exists(), \"Place era5_ireland3_t2m_wind_2024.csv next to this notebook.\"\n",
    "print(\"Found CSV ✓\")\n",
    "device = \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db2fb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load & quick look\n",
    "df = pd.read_csv(CSV_PATH, parse_dates=['time']).sort_values('time').reset_index(drop=True)\n",
    "print(df.head(5))\n",
    "print(\"Coverage:\", df['time'].min(), \"→\", df['time'].max(), f\"({len(df)} hourly rows)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd92ccaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time split: 70/15/15\n",
    "n = len(df)\n",
    "i_tr = int(0.70*n); i_va = int(0.85*n)\n",
    "splits = {'train': (0, i_tr), 'val': (i_tr, i_va), 'test': (i_va, n)}\n",
    "print(splits)\n",
    "\n",
    "# Build 24->next windows on scaled Dublin temp\n",
    "T = 24\n",
    "series = df[['Dublin_t2m_degC']].copy()\n",
    "sc = StandardScaler().fit(series.iloc[splits['train'][0]:splits['train'][1]])\n",
    "x_all = sc.transform(series.values)  # (N,1)\n",
    "\n",
    "def make_windows(x, T=24):\n",
    "    Xs, ys = [], []\n",
    "    for t in range(len(x)-T):\n",
    "        Xs.append(x[t:t+T])\n",
    "        ys.append(x[t+T])\n",
    "    return np.stack(Xs), np.stack(ys)\n",
    "\n",
    "X_all, y_all = make_windows(x_all, T)\n",
    "M = len(X_all); i_tr_w = int(0.70*M); i_va_w = int(0.85*M)\n",
    "Xtr, ytr = X_all[:i_tr_w], y_all[:i_tr_w]\n",
    "Xva, yva = X_all[i_tr_w:i_va_w], y_all[i_tr_w:i_va_w]\n",
    "Xte, yte = X_all[i_va_w:], y_all[i_va_w:]\n",
    "print(\"Shapes:\", Xtr.shape, ytr.shape, \"|\", Xva.shape, yva.shape, \"|\", Xte.shape, yte.shape)\n",
    "\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "    def __len__(self): return len(self.X)\n",
    "    def __getitem__(self, i): return self.X[i], self.y[i]\n",
    "\n",
    "tr_dl = DataLoader(SeqDataset(Xtr,ytr), batch_size=128, shuffle=True)\n",
    "va_dl = DataLoader(SeqDataset(Xva,yva), batch_size=256)\n",
    "te_dl = DataLoader(SeqDataset(Xte,yte), batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee218f93",
   "metadata": {},
   "source": [
    "### Model 1: Vanilla RNN (`nn.RNN`)\n",
    "\n",
    "Small hidden size, 8 epochs, gradient clipping. We'll compare to LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b7021d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyRNN(nn.Module):\n",
    "    def __init__(self, hidden=16):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(1, hidden, nonlinearity='tanh', batch_first=True)\n",
    "        self.head = nn.Linear(hidden, 1)\n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)\n",
    "        return self.head(out[:,-1,:])\n",
    "\n",
    "torch.manual_seed(0)\n",
    "model_rnn = TinyRNN()\n",
    "opt = torch.optim.Adam(model_rnn.parameters(), lr=3e-3)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "def eval_mse(model, loader):\n",
    "    model.eval(); tot=0; n=0\n",
    "    with torch.no_grad():\n",
    "        for xb,yb in loader:\n",
    "            yh = model(xb)\n",
    "            l = loss_fn(yh, yb).item()\n",
    "            tot += l*len(xb); n += len(xb)\n",
    "    return tot/n\n",
    "\n",
    "for ep in range(8):\n",
    "    model_rnn.train(); run=0; ntr=0\n",
    "    for xb,yb in tr_dl:\n",
    "        opt.zero_grad()\n",
    "        yh = model_rnn(xb)\n",
    "        loss = loss_fn(yh, yb)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model_rnn.parameters(), 1.0)\n",
    "        opt.step()\n",
    "        run += loss.item()*len(xb); ntr += len(xb)\n",
    "    val = eval_mse(model_rnn, va_dl)\n",
    "    print(f\"Epoch {ep+1:02d} | train MSE={run/ntr:.4f} | val MSE={val:.4f}\")\n",
    "\n",
    "# Test RMSE back in °C\n",
    "def rmse_c(model, loader, scaler):\n",
    "    model.eval(); P=[]; T_=[]\n",
    "    with torch.no_grad():\n",
    "        for xb,yb in loader:\n",
    "            P.append(model(xb).numpy()); T_.append(yb.numpy())\n",
    "    P = np.vstack(P); T_ = np.vstack(T_)\n",
    "    P_inv = scaler.inverse_transform(P); T_inv = scaler.inverse_transform(T_)\n",
    "    return float(np.sqrt(((P_inv - T_inv)**2).mean())), P_inv, T_inv\n",
    "\n",
    "rmse_rnn, pred_rnn, true_rnn = rmse_c(model_rnn, te_dl, sc)\n",
    "print(f\"Test RMSE — RNN: {rmse_rnn:.3f} °C\")\n",
    "\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.plot(true_rnn[:200,0], label='Truth')\n",
    "plt.plot(pred_rnn[:200,0], label='RNN pred')\n",
    "plt.title('RNN — Dublin t2m (test segment)'); plt.legend(); plt.grid(True); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5a8443",
   "metadata": {},
   "source": [
    "### Model 2: LSTM (`nn.LSTM`)\n",
    "\n",
    "Same task/setup as the RNN so results are comparable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab73439c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyLSTM(nn.Module):\n",
    "    def __init__(self, hidden=32):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.LSTM(1, hidden, batch_first=True)\n",
    "        self.head = nn.Linear(hidden, 1)\n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)\n",
    "        return self.head(out[:,-1,:])\n",
    "\n",
    "torch.manual_seed(0)\n",
    "model_lstm = TinyLSTM()\n",
    "opt = torch.optim.Adam(model_lstm.parameters(), lr=3e-3)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "for ep in range(8):\n",
    "    model_lstm.train(); run=0; ntr=0\n",
    "    for xb,yb in tr_dl:\n",
    "        opt.zero_grad()\n",
    "        yh = model_lstm(xb)\n",
    "        loss = loss_fn(yh, yb)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model_lstm.parameters(), 1.0)\n",
    "        opt.step()\n",
    "        run += loss.item()*len(xb); ntr += len(xb)\n",
    "    val = eval_mse(model_lstm, va_dl)\n",
    "    print(f\"Epoch {ep+1:02d} | train MSE={run/ntr:.4f} | val MSE={val:.4f}\")\n",
    "\n",
    "rmse_lstm, pred_lstm, true_lstm = rmse_c(model_lstm, te_dl, sc)\n",
    "print(f\"Test RMSE — LSTM: {rmse_lstm:.3f} °C\")\n",
    "\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.plot(true_lstm[:200,0], label='Truth')\n",
    "plt.plot(pred_lstm[:200,0], label='LSTM pred')\n",
    "plt.title('LSTM — Dublin t2m (test segment)'); plt.legend(); plt.grid(True); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fceb1116",
   "metadata": {},
   "source": [
    "**Takeaways (from the slides):**\n",
    "- RNNs can struggle with long dependencies (vanishing/exploding gradients).\n",
    "- LSTMs add gates + cell state to help remember/forget information.\n",
    "- Always: standardise inputs, use sensible window lengths, clip gradients, watch val loss.\n",
    "\n",
    "**Try next:** change `T` to 12/48; train for 20 epochs; add Dublin wind as a second feature."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
