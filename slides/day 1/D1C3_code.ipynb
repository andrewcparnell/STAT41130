{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9eeeaeff",
   "metadata": {},
   "source": [
    "# D1C3 Code (Notebook version)\n",
    "\n",
    "- Converted automatically on 2025-09-18 18:43:44.\n",
    "- Original source: `D1C3_code.py`.\n",
    "\n",
    "Use **Run All** or run cells step by step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bc97a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: environment check (uncomment pip lines if needed)\n",
    "# !pip install -q numpy matplotlib torch\n",
    "\n",
    "import sys\n",
    "print(sys.version)\n",
    "try:\n",
    "    import numpy, matplotlib, torch  # noqa: F401\n",
    "    print(\"✅ numpy/matplotlib/torch found\")\n",
    "except Exception as e:\n",
    "    print(\"⚠️ Missing package:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76601e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "STAT41130 — AI for Weather & Climate\n",
    "Day 1 Companion Script (Lectures: Linear Regression → Neural Networks, forward-pass focus)\n",
    "\n",
    "Last updated: 2025-09-18\n",
    "\n",
    "What this script is:\n",
    "    A heavily commented, line-by-line teaching companion for the first two lectures.\n",
    "    It takes you from:\n",
    "        * dot products and linear transforms,\n",
    "        * to linear regression as a single-neuron network,\n",
    "        * to activations (sigmoid, tanh, ReLU),\n",
    "        * to multi-feature inputs and matrix shapes,\n",
    "        * and finally to a small taste of PyTorch and autograd.\n",
    "\n",
    "How to use:\n",
    "    - Run this script top-to-bottom. Skim comments first, then re-run slowly, line-by-line.\n",
    "    - Search for the tag 'EXERCISE' — those blocks are designed to pause & discuss.\n",
    "    - You can tweak knobs (learning rate, epochs, initial weights) and see the effect.\n",
    "    - Plots are optional but nice; they use matplotlib (no custom styles set, per course rules).\n",
    "\n",
    "Why so many comments?\n",
    "    The aim is clarity. Every step is annotated so you can connect the lecture slides to code.\n",
    "\n",
    "Note:\n",
    "    We intentionally do not use PyTorch until Section 10 to ensure you understand the core math.\n",
    "    Early sections use only Python and NumPy.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ee7e5a",
   "metadata": {},
   "source": [
    "============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a42571",
   "metadata": {},
   "source": [
    "Section 0 — Imports, printing helpers, reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be03330",
   "metadata": {},
   "source": [
    "============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab45e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import sys\n",
    "import math\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, Callable, Iterable, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Matplotlib is used for a few simple 1-figure plots (one chart per figure; no custom colors).\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# We will only import torch later (Section 10) so that the early parts are framework-free.\n",
    "\n",
    "# For reproducibility in random demos:\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "\n",
    "def header(title: str):\n",
    "    \"\"\"Pretty-print a section header so it's obvious in the console.\"\"\"\n",
    "    line = \"=\" * len(title)\n",
    "    print(f\"\\n{line}\\n{title}\\n{line}\\n\")\n",
    "\n",
    "\n",
    "def subheader(title: str):\n",
    "    \"\"\"Pretty-print a sub-section header.\"\"\"\n",
    "    line = \"-\" * len(title)\n",
    "    print(f\"\\n{title}\\n{line}\")\n",
    "\n",
    "\n",
    "def show_vector(name: str, v: np.ndarray):\n",
    "    \"\"\"Display a 1-D vector with its shape (convenience for teaching).\"\"\"\n",
    "    print(f\"{name} (shape {v.shape}): {np.array2string(v, precision=4, floatmode='fixed')}\")\n",
    "\n",
    "\n",
    "def show_matrix(name: str, M: np.ndarray):\n",
    "    \"\"\"Display a 2-D matrix with its shape (convenience for teaching).\"\"\"\n",
    "    with np.printoptions(precision=4, suppress=True, floatmode='fixed'):\n",
    "        print(f\"{name} (shape {M.shape}):\\n{M}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e842b732",
   "metadata": {},
   "source": [
    "============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188db61a",
   "metadata": {},
   "source": [
    "Section 1 — The tiniest 'weather' dataset & dot product warm‑up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88c6288",
   "metadata": {},
   "source": [
    "============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683866e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "header(\"Section 1 — Tiny 'weather' dataset & dot product\")\n",
    "\n",
    "# We'll reuse the 5-point temperature example from the lecture slides:\n",
    "# X = temperature yesterday, y = temperature today\n",
    "X = np.array([16.09, 15.56, 15.85, 15.69, 15.01], dtype=float)\n",
    "y = np.array([17.62, 14.88, 16.32, 16.28, 14.96], dtype=float)\n",
    "\n",
    "show_vector(\"X (yesterday °C)\", X)\n",
    "show_vector(\"y (today °C)\", y)\n",
    "\n",
    "# Dot product refresher (forward step inside a neuron):\n",
    "x = np.array([2.0, -3.0, 0.5])  # toy 3-d input\n",
    "w = np.array([0.1, 0.2, 0.3])   # toy weights\n",
    "b = 0.05                        # bias\n",
    "\n",
    "dot = x @ w  # same as np.dot(x, w)\n",
    "z = dot + b  # pre-activation (a.k.a. linear transform)\n",
    "print(f\"\\nToy dot product: x @ w = {dot:.4f}, plus bias => z = {z:.4f}\")\n",
    "\n",
    "# EXERCISE (verbal): Change x, w, b and predict how z changes before running the code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91d033b",
   "metadata": {},
   "source": [
    "============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcb7d2b",
   "metadata": {},
   "source": [
    "Section 2 — Linear regression as a single-neuron network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d1296a",
   "metadata": {},
   "source": [
    "============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8af5edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "header(\"Section 2 — Linear regression as a single neuron\")\n",
    "\n",
    "# A linear regression with 1 feature can be seen as:\n",
    "# y_hat = w * X + b  where w,b are parameters to learn.\n",
    "\n",
    "def predict_lr_1d(X: np.ndarray, w: float, b: float) -> np.ndarray:\n",
    "    \"\"\"Forward pass for 1D linear regression.\"\"\"\n",
    "    return w * X + b\n",
    "\n",
    "\n",
    "def rmse(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    \"\"\"Root Mean Squared Error — often interpretable in the same units as y.\"\"\"\n",
    "    return float(np.sqrt(np.mean((y_true - y_pred) ** 2)))\n",
    "\n",
    "\n",
    "# Let's try the two candidates from the slides to build intuition.\n",
    "candidates = [\n",
    "    dict(w=0.0, b=0.5),\n",
    "    dict(w=1.0, b=1.0),\n",
    "]\n",
    "\n",
    "for c in candidates:\n",
    "    yhat = predict_lr_1d(X, c[\"w\"], c[\"b\"])\n",
    "    loss = rmse(y, yhat)\n",
    "    print(f\"Candidate (w={c['w']:.2f}, b={c['b']:.2f}) → RMSE = {loss:.4f}\")\n",
    "\n",
    "# The second pair should be dramatically better on these 5 points (as shown in class)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7040635",
   "metadata": {},
   "source": [
    "============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0009f2c7",
   "metadata": {},
   "source": [
    "Section 3 — Visual intuition: plotting the fit for a few (w, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04380c9c",
   "metadata": {},
   "source": [
    "============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31a4088",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "header(\"Section 3 — Visualising a few candidate lines\")\n",
    "\n",
    "def plot_candidates(X: np.ndarray, y: np.ndarray, wb_list: List[Tuple[float, float]]):\n",
    "    \"\"\"\n",
    "    Show X vs y and a few candidate regression lines.\n",
    "    Rule for plots in this course:\n",
    "        * matplotlib only\n",
    "        * one chart per figure\n",
    "        * do not set any specific colors or styles\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.scatter(X, y, label=\"Data\")\n",
    "    xgrid = np.linspace(X.min()-0.5, X.max()+0.5, 100)\n",
    "    for w, b in wb_list:\n",
    "        plt.plot(xgrid, w * xgrid + b, label=f\"w={w:.2f}, b={b:.2f}\")\n",
    "    plt.xlabel(\"Temperature yesterday (°C)\")\n",
    "    plt.ylabel(\"Temperature today (°C)\")\n",
    "    plt.title(\"Linear regression — candidate fits\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_candidates(X, y, wb_list=[(0.0, 0.5), (1.0, 1.0), (1.2, 0.0)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd84797c",
   "metadata": {},
   "source": [
    "============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a8b5e5",
   "metadata": {},
   "source": [
    "Section 4 — Loss landscapes by brute force (grid search)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02376d11",
   "metadata": {},
   "source": [
    "============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2f485d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "header(\"Section 4 — Loss landscape via grid search\")\n",
    "\n",
    "def grid_search_rmse(X: np.ndarray, y: np.ndarray,\n",
    "                     w_values: np.ndarray, b_values: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Return a 2D array L[i,j] = RMSE at w[i], b[j].\"\"\"\n",
    "    L = np.zeros((len(w_values), len(b_values)), dtype=float)\n",
    "    for i, w in enumerate(w_values):\n",
    "        for j, b in enumerate(b_values):\n",
    "            L[i, j] = rmse(y, predict_lr_1d(X, w, b))\n",
    "    return L\n",
    "\n",
    "\n",
    "w_grid = np.linspace(0.0, 2.0, 41)  # 41 points in [0, 2]\n",
    "b_grid = np.linspace(-1.0, 1.0, 41)\n",
    "L = grid_search_rmse(X, y, w_grid, b_grid)\n",
    "\n",
    "# Visualize as a contour plot (one figure, no custom colors).\n",
    "plt.figure()\n",
    "W, B = np.meshgrid(w_grid, b_grid, indexing=\"ij\")\n",
    "CS = plt.contour(W, B, L, levels=15)\n",
    "plt.clabel(CS, inline=True, fontsize=8)\n",
    "plt.xlabel(\"w\")\n",
    "plt.ylabel(\"b\")\n",
    "plt.title(\"RMSE loss landscape — 1D linear regression\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2332e65d",
   "metadata": {},
   "source": [
    "============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7479e3",
   "metadata": {},
   "source": [
    "Section 5 — Gradient descent for 1D linear regression (by hand)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf8c357",
   "metadata": {},
   "source": [
    "============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ace0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "header(\"Section 5 — Gradient descent for 1D LR (by hand)\")\n",
    "\n",
    "def compute_gradients_1d(X: np.ndarray, y: np.ndarray, w: float, b: float) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Compute gradients of MSE loss (not RMSE) w.r.t w and b.\n",
    "    We can use MSE because it's simpler for derivatives; updating under MSE or RMSE is similar near optimum.\n",
    "    d/dw MSE = (2/n) * sum( (w*X + b - y) * X )\n",
    "    d/db MSE = (2/n) * sum( (w*X + b - y) )\n",
    "    \"\"\"\n",
    "    n = len(X)\n",
    "    yhat = w * X + b\n",
    "    residual = yhat - y\n",
    "    dw = (2.0 / n) * np.sum(residual * X)\n",
    "    db = (2.0 / n) * np.sum(residual)\n",
    "    return float(dw), float(db)\n",
    "\n",
    "\n",
    "def gradient_descent_1d(X: np.ndarray, y: np.ndarray, w0: float, b0: float,\n",
    "                        lr: float = 0.01, epochs: int = 100) -> Tuple[float, float, List[float]]:\n",
    "    \"\"\"Run gradient descent and return final (w, b, loss_history).\"\"\"\n",
    "    w, b = float(w0), float(b0)\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        dw, db = compute_gradients_1d(X, y, w, b)\n",
    "        w -= lr * dw\n",
    "        b -= lr * db\n",
    "        loss = rmse(y, w * X + b)\n",
    "        losses.append(loss)\n",
    "        if epoch % max(1, epochs // 10) == 0:\n",
    "            print(f\"Epoch {epoch:4d} | RMSE: {loss:.6f} | w: {w:.6f} | b: {b:.6f}\")\n",
    "    return w, b, losses\n",
    "\n",
    "\n",
    "# Initialize at zeros:\n",
    "w_init, b_init = 0.0, 0.0\n",
    "w_hat, b_hat, loss_hist = gradient_descent_1d(X, y, w_init, b_init, lr=0.01, epochs=120)\n",
    "\n",
    "# Plot the training curve (single figure).\n",
    "plt.figure()\n",
    "plt.plot(np.arange(len(loss_hist)), loss_hist)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.title(\"Gradient descent learning curve (1D LR)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac3149f",
   "metadata": {},
   "source": [
    "============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a2bb28",
   "metadata": {},
   "source": [
    "Section 6 — From 1 feature to many: vector & matrix forms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450c3977",
   "metadata": {},
   "source": [
    "============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d38438e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "header(\"Section 6 — Multi-feature LR & shapes\")\n",
    "\n",
    "# Let's construct a toy feature matrix with 2 inputs (e.g., temperature and humidity yesterday).\n",
    "# We'll create tiny synthetic data to keep focus on the shapes.\n",
    "n_samples = 5\n",
    "X2 = np.column_stack([\n",
    "    X,                                  # feature 1 = temperature yesterday\n",
    "    np.array([65, 70, 68, 72, 60], float)  # feature 2 = humidity yesterday (%)\n",
    "])  # shape: (5, 2)\n",
    "\n",
    "# We'll pretend \"today's temperature\" depends on both features linearly.\n",
    "# For demo, set up pseudo-true parameters:\n",
    "w_true = np.array([1.02, -0.03])   # weights for (temp, humidity)\n",
    "b_true = 0.06                      # small bias\n",
    "\n",
    "y2 = X2 @ w_true + b_true  # forward pass in vector form\n",
    "show_matrix(\"X2\", X2)\n",
    "show_vector(\"w_true\", w_true)\n",
    "print(f\"b_true: {b_true:.4f}\")\n",
    "show_vector(\"y2 = X2 @ w_true + b_true\", y2)\n",
    "\n",
    "# EXERCISE: Check shapes. X2 is (5,2); w_true is (2,); bias is scalar. Why does broadcasting work here?\n",
    "\n",
    "def predict_lr_multi(X: np.ndarray, w: np.ndarray, b: float) -> np.ndarray:\n",
    "    \"\"\"Vectorized forward pass for multi-feature linear regression.\"\"\"\n",
    "    return X @ w + b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28f8956",
   "metadata": {},
   "source": [
    "============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e3b531",
   "metadata": {},
   "source": [
    "Section 7 — Binary classification: logistic regression forward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d6bc74",
   "metadata": {},
   "source": [
    "============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4fe1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "header(\"Section 7 — Binary classification & sigmoid\")\n",
    "\n",
    "def sigmoid(z: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Logistic sigmoid function.\"\"\"\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "\n",
    "def binary_cross_entropy(y_true: np.ndarray, y_prob: np.ndarray, eps: float = 1e-15) -> float:\n",
    "    \"\"\"\n",
    "    Binary cross-entropy loss.\n",
    "    We clip probabilities for numerical stability to avoid log(0).\n",
    "    \"\"\"\n",
    "    y_prob = np.clip(y_prob, eps, 1.0 - eps)\n",
    "    return float(-np.mean(y_true * np.log(y_prob) + (1 - y_true) * np.log(1 - y_prob)))\n",
    "\n",
    "\n",
    "# Build a tiny synthetic binary dataset with two features (x1, x2), and a linearly separable boundary.\n",
    "rng = np.random.default_rng(0)\n",
    "N = 60\n",
    "x1 = rng.normal(loc=0.0, scale=1.0, size=N)\n",
    "x2 = rng.normal(loc=0.0, scale=1.0, size=N)\n",
    "Xb = np.column_stack([x1, x2])\n",
    "# True underlying separator: 0.7 * x1 - 0.5 * x2 + 0.2 > 0 ⇒ class 1\n",
    "ybin = (0.7 * x1 - 0.5 * x2 + 0.2 > 0).astype(float)\n",
    "\n",
    "# Forward pass with random weights:\n",
    "w_log = rng.normal(size=2)\n",
    "b_log = 0.0\n",
    "z = Xb @ w_log + b_log\n",
    "p = sigmoid(z)\n",
    "bce = binary_cross_entropy(ybin, p)\n",
    "print(f\"Random-init logistic regression → BCE = {bce:.4f}\")\n",
    "\n",
    "# NOTE: We aren't training yet — just connecting the forward step to probabilities in (0,1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e21d50f",
   "metadata": {},
   "source": [
    "============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863af6ed",
   "metadata": {},
   "source": [
    "Section 8 — Activation functions: sigmoid, tanh, ReLU (by hand)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232b5df9",
   "metadata": {},
   "source": [
    "============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec43e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "header(\"Section 8 — Activation functions in action\")\n",
    "\n",
    "def relu(z: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"ReLU activation.\"\"\"\n",
    "    return np.maximum(0.0, z)\n",
    "\n",
    "\n",
    "def tanh(z: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Hyperbolic tangent.\"\"\"\n",
    "    return np.tanh(z)\n",
    "\n",
    "\n",
    "# Demo: apply activations to a simple pre-activation vector\n",
    "z_demo = np.linspace(-3, 3, 13)\n",
    "show_vector(\"z_demo\", z_demo)\n",
    "show_vector(\"sigmoid(z_demo)\", sigmoid(z_demo))\n",
    "show_vector(\"tanh(z_demo)\", tanh(z_demo))\n",
    "show_vector(\"relu(z_demo)\", relu(z_demo))\n",
    "\n",
    "# Plot the three activation curves on separate figures (course rule: one chart per figure).\n",
    "plt.figure()\n",
    "plt.plot(z_demo, sigmoid(z_demo))\n",
    "plt.xlabel(\"z\")\n",
    "plt.ylabel(\"sigmoid(z)\")\n",
    "plt.title(\"Sigmoid activation\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(z_demo, tanh(z_demo))\n",
    "plt.xlabel(\"z\")\n",
    "plt.ylabel(\"tanh(z)\")\n",
    "plt.title(\"Tanh activation\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(z_demo, relu(z_demo))\n",
    "plt.xlabel(\"z\")\n",
    "plt.ylabel(\"ReLU(z)\")\n",
    "plt.title(\"ReLU activation\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5905e9",
   "metadata": {},
   "source": [
    "============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241fca26",
   "metadata": {},
   "source": [
    "Section 9 — A tiny 2-layer network: manual forward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a4233b",
   "metadata": {},
   "source": [
    "============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1778d0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "header(\"Section 9 — Manual forward for a tiny 2-layer NN\")\n",
    "\n",
    "# We'll build a 2-2-1 network (2 inputs → 2 hidden (ReLU) → 1 output (linear)).\n",
    "# Shapes:\n",
    "#   W1: (2, 2), b1: (2,)\n",
    "#   W2: (2, 1), b2: (1,)\n",
    "\n",
    "def forward_2_2_1(x: np.ndarray,\n",
    "                  W1: np.ndarray, b1: np.ndarray,\n",
    "                  W2: np.ndarray, b2: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Manual forward pass for one sample x (shape: (2,)).\n",
    "    \"\"\"\n",
    "    h_pre = x @ W1 + b1           # shape (2,)\n",
    "    h = relu(h_pre)               # activation\n",
    "    yhat = h @ W2 + b2            # scalar\n",
    "    return float(yhat)\n",
    "\n",
    "\n",
    "# Make up a simple example:\n",
    "x_sample = np.array([0.3, -1.2])\n",
    "W1 = np.array([[ 0.5, -0.4],\n",
    "               [ 1.1,  0.2]])\n",
    "b1 = np.array([0.0, -0.1])\n",
    "W2 = np.array([[ 0.7],\n",
    "               [-1.0]])\n",
    "b2 = np.array([0.05])\n",
    "\n",
    "yhat_ex = forward_2_2_1(x_sample, W1, b1, W2, b2)\n",
    "print(f\"Manual forward output for x={x_sample}: y_hat = {yhat_ex:.4f}\")\n",
    "\n",
    "# EXERCISE: Zero out b1 and b2. Re-run. What's the effect of removing biases?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1936f75",
   "metadata": {},
   "source": [
    "============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeabfaae",
   "metadata": {},
   "source": [
    "Section 10 — Enter PyTorch: tensors, autograd, and fitting LR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf3a633",
   "metadata": {},
   "source": [
    "============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78239baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "header(\"Section 10 — PyTorch: tensors & autograd (linear regression)\")\n",
    "\n",
    "# We delay importing torch until now to keep early sections math-first.\n",
    "import torch\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Convert our original small dataset to torch tensors.\n",
    "X_t = torch.tensor(X, dtype=torch.float32).unsqueeze(1)  # shape (5,1)\n",
    "y_t = torch.tensor(y, dtype=torch.float32).unsqueeze(1)  # shape (5,1)\n",
    "\n",
    "# Define a minimal linear model: y = w*X + b\n",
    "model_lr = torch.nn.Linear(in_features=1, out_features=1, bias=True)\n",
    "\n",
    "# Mean Squared Error loss and a basic optimizer (SGD).\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model_lr.parameters(), lr=0.05)\n",
    "\n",
    "# Train loop — VERY small, for demonstration.\n",
    "epochs = 200\n",
    "losses_torch = []\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model_lr(X_t)\n",
    "    loss = criterion(y_pred, y_t)\n",
    "    loss.backward()       # autograd computes d(loss)/d(params)\n",
    "    optimizer.step()      # update parameters\n",
    "    losses_torch.append(float(torch.sqrt(loss).item()))  # store RMSE\n",
    "    if epoch % 20 == 0:\n",
    "        w_val = model_lr.weight.item()\n",
    "        b_val = model_lr.bias.item()\n",
    "        print(f\"[Torch LR] Epoch {epoch:3d} | RMSE ~ {math.sqrt(loss.item()):.6f} | w: {w_val:.6f} | b: {b_val:.6f}\")\n",
    "\n",
    "# Plot the PyTorch learning curve (single figure).\n",
    "plt.figure()\n",
    "plt.plot(np.arange(len(losses_torch)), losses_torch)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.title(\"PyTorch linear regression — learning curve\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Inspect learned parameters.\n",
    "w_learned = model_lr.weight.detach().cpu().numpy().squeeze()\n",
    "b_learned = model_lr.bias.detach().cpu().numpy().squeeze()\n",
    "print(f\"Learned parameters (PyTorch): w ≈ {w_learned:.4f}, b ≈ {b_learned:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8d2184",
   "metadata": {},
   "source": [
    "============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cd92d3",
   "metadata": {},
   "source": [
    "Section 11 — PyTorch: a tiny MLP for binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca96b51",
   "metadata": {},
   "source": [
    "============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c582d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "header(\"Section 11 — PyTorch MLP (2→3→1) with ReLU on synthetic binary data\")\n",
    "\n",
    "# We'll reuse Xb (N x 2) and ybin (N) from Section 7 but convert to tensors.\n",
    "Xb_t = torch.tensor(Xb, dtype=torch.float32)\n",
    "ybin_t = torch.tensor(ybin, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "class TinyMLP(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A tiny multilayer perceptron:\n",
    "        input 2 → hidden 3 (ReLU) → output 1 (logit)\n",
    "    We'll use BCEWithLogitsLoss which expects raw scores (\"logits\"),\n",
    "    and combines sigmoid + BCE in a stable way.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = torch.nn.Linear(2, 3)\n",
    "        self.fc2 = torch.nn.Linear(3, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = torch.relu(self.fc1(x))\n",
    "        logit = self.fc2(h)\n",
    "        return logit\n",
    "\n",
    "mlp = TinyMLP()\n",
    "criterion_bce = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer_mlp = torch.optim.Adam(mlp.parameters(), lr=0.05)\n",
    "\n",
    "epochs = 300\n",
    "loss_hist_mlp = []\n",
    "for epoch in range(epochs):\n",
    "    optimizer_mlp.zero_grad()\n",
    "    logits = mlp(Xb_t)\n",
    "    loss = criterion_bce(logits, ybin_t)\n",
    "    loss.backward()\n",
    "    optimizer_mlp.step()\n",
    "    loss_hist_mlp.append(float(loss.item()))\n",
    "    if epoch % 50 == 0:\n",
    "        with torch.no_grad():\n",
    "            probs = torch.sigmoid(logits)\n",
    "            preds = (probs > 0.5).float()\n",
    "            acc = (preds.eq(ybin_t).float().mean().item())\n",
    "        print(f\"[MLP] Epoch {epoch:3d} | loss: {loss.item():.4f} | acc: {acc:.3f}\")\n",
    "\n",
    "# Plot training loss (single figure).\n",
    "plt.figure()\n",
    "plt.plot(np.arange(len(loss_hist_mlp)), loss_hist_mlp)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"BCE loss\")\n",
    "plt.title(\"Tiny MLP — training loss\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25eb498",
   "metadata": {},
   "source": [
    "============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2926a4c1",
   "metadata": {},
   "source": [
    "Section 12 — Sanity checks & common pitfalls (from the slides)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdaa632",
   "metadata": {},
   "source": [
    "============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcdb1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "header(\"Section 12 — Sanity checks & pitfalls\")\n",
    "\n",
    "checks = [\n",
    "    \"Check shapes at every layer (especially batch dimension).\",\n",
    "    \"Do not forget the bias terms.\",\n",
    "    \"Use appropriate activation for the task (e.g., none/linear for regression, sigmoid/logits for binary).\",\n",
    "    \"Normalize or standardize inputs when features are on very different scales.\",\n",
    "    \"Monitor a validation split if data volume allows; early stopping helps.\",\n",
    "    \"Learning rate matters. Too big → diverge; too small → crawl.\",\n",
    "]\n",
    "for i, c in enumerate(checks, 1):\n",
    "    print(f\"{i}. {c}\")\n",
    "\n",
    "# EXERCISE: Print the parameter counts for TinyMLP and discuss how they scale.\n",
    "def count_parameters(model: torch.nn.Module) -> int:\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"TinyMLP trainable parameters: {count_parameters(mlp)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2c67e5",
   "metadata": {},
   "source": [
    "============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d49144c",
   "metadata": {},
   "source": [
    "Section 13 — Mini-lab ideas (optional explorations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436f9d2a",
   "metadata": {},
   "source": [
    "============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b484a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "header(\"Section 13 — Mini-lab ideas\")\n",
    "\n",
    "ideas = [\n",
    "    \"Swap ReLU for tanh in the hidden layer. What changes?\",\n",
    "    \"Standardize x1,x2 before feeding the MLP; rerun and compare convergence.\",\n",
    "    \"Create a new synthetic decision boundary and see if the MLP can learn it.\",\n",
    "    \"Back in Section 5, try different learning rates (1e-3 to 1e-1) and plot learning curves.\",\n",
    "    \"Add L2 weight decay in the PyTorch optimizers (weight_decay=1e-4) and note effects.\",\n",
    "]\n",
    "for i, idea in enumerate(ideas, 1):\n",
    "    print(f\"({i}) {idea}\")\n",
    "\n",
    "print(\"\\nAll done 🎉  You have stepped from dot products → linear models → activations → tiny NNs,\")\n",
    "print(\"and you've seen both from-scratch math and PyTorch's autograd in action.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
