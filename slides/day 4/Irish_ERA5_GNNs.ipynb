{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8106db77",
   "metadata": {},
   "source": [
    "# Irish Weather — Graph Neural Networks (GCN & GAT) on ERA5 (Dublin, Galway, Cork)\n",
    "\n",
    "This notebook follows the **Graph Neural Networks** lecture and uses the same CSV as the previous notebooks:\n",
    "`era5_ireland3_t2m_wind_2024.csv`\n",
    "\n",
    "We build a small **3-node graph** for **Dublin (0), Galway (1), Cork (2)** and link neighbours:\n",
    "- Edges: Dublin ↔ Galway, Galway ↔ Cork (plus self-loops).  \n",
    "- Tasks (in increasing complexity):\n",
    "  1. **Manual GCN step (NumPy)** — reproduce the slide's propagation rule with tiny numbers.\n",
    "  2. **GCN (PyTorch Geometric)** — predict next-hour **temperature** for each city from the **last 24 hours** (per city).\n",
    "  3. **GCN + simple temporal encoder** — add a tiny 1D Conv encoder over the last 24 h sequence before the GCN.\n",
    "  4. **GAT (Graph Attention)** — final example using **all variables** (t2m + wind at all cities) with multi-head attention.\n",
    "\n",
    "We keep code short, CPU-friendly, and heavily commented."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0d400d",
   "metadata": {},
   "source": [
    "## 0) Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "becfc341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# If needed, install packages (uncomment appropriate lines).\n",
    "# %pip install pandas numpy matplotlib scikit-learn torch\n",
    "# PyTorch Geometric install depends on your torch + CUDA/CPU version.\n",
    "# For CPU-only (example for torch>=2.4):\n",
    "# %pip install --no-index torch-scatter torch-sparse torch-cluster torch-spline-conv -f https://data.pyg.org/whl/torch-2.4.0+cpu.html\n",
    "# %pip install torch-geometric\n",
    "\n",
    "import warnings, pathlib\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "try:\n",
    "    from torch_geometric.data import Data\n",
    "    from torch_geometric.nn import GCNConv, GATConv\n",
    "    PYG_OK = True\n",
    "except Exception as e:\n",
    "    PYG_OK = False\n",
    "    print(\"PyTorch Geometric not found. GCN/GAT sections will error if you run them. See install instructions above.\")\n",
    "\n",
    "CSV_PATH = \"../../data/era5_ireland3_t2m_wind_2024.csv\"\n",
    "assert pathlib.Path(CSV_PATH).exists(), \"Place the CSV next to this notebook.\"\n",
    "device = \"cpu\"\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84c6a48",
   "metadata": {},
   "source": [
    "## 1) Load ERA5 and define graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cd9af69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 time  Dublin_t2m_degC  Galway_t2m_degC  Cork_t2m_degC  \\\n",
      "0 2024-01-01 00:00:00         5.763580         6.396393       5.617096   \n",
      "1 2024-01-01 01:00:00         5.445953         5.922516       5.094391   \n",
      "2 2024-01-01 02:00:00         5.434723         5.518707       4.604645   \n",
      "\n",
      "   Dublin_wind_speed10m_ms  Galway_wind_speed10m_ms  Cork_wind_speed10m_ms  \n",
      "0                 8.426119                 6.120335               4.927529  \n",
      "1                 8.082199                 5.639297               4.454760  \n",
      "2                 7.943061                 4.928046               3.922362  \n",
      "Coverage: 2024-01-01 00:00:00 → 2024-12-31 23:00:00 (8784 hourly rows)\n",
      "edge_index shape: torch.Size([2, 14]) → E= 14\n"
     ]
    }
   ],
   "source": [
    "# Load\n",
    "df = pd.read_csv(CSV_PATH, parse_dates=['time']).sort_values('time').reset_index(drop=True)\n",
    "print(df.head(3))\n",
    "print(\"Coverage:\", df['time'].min(), '→', df['time'].max(), f\"({len(df)} hourly rows)\")\n",
    "\n",
    "# Node order: Dublin(0), Galway(1), Cork(2)\n",
    "CITY_COLS_T2M = ['Dublin_t2m_degC','Galway_t2m_degC','Cork_t2m_degC']\n",
    "CITY_COLS_WS  = ['Dublin_wind_speed10m_ms','Galway_wind_speed10m_ms','Cork_wind_speed10m_ms']\n",
    "\n",
    "# Graph edges (undirected chain with self-loops): 0-1-2\n",
    "edge_index = torch.tensor([\n",
    "    [0,1, 1,0, 1,2, 2,1, 0,0, 1,1, 2,2],  # src\n",
    "    [1,0, 2,1, 0,1, 1,2, 0,0, 1,1, 2,2],  # dst\n",
    "], dtype=torch.long)  # shape (2, E)\n",
    "print(\"edge_index shape:\", edge_index.shape, \"→ E=\", edge_index.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8fc69f",
   "metadata": {},
   "source": [
    "## 2) Manual GCN step (NumPy) — matches the slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c117d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_norm=\n",
      " [[0.5   0.408 0.   ]\n",
      " [0.408 0.333 0.408]\n",
      " [0.    0.408 0.5  ]]\n",
      "H1=\n",
      " [[1.975]\n",
      " [3.449]\n",
      " [3.475]]\n"
     ]
    }
   ],
   "source": [
    "# Tiny 3-node example like on the slides.\n",
    "A = np.array([[0,1,0],\n",
    "              [1,0,1],\n",
    "              [0,1,0]], dtype=float)\n",
    "I = np.eye(3)\n",
    "A_hat = A + I\n",
    "D = np.diag(A_hat.sum(axis=1))\n",
    "D_inv_sqrt = np.linalg.inv(np.sqrt(D))\n",
    "A_norm = D_inv_sqrt @ A_hat @ D_inv_sqrt  # normalized adjacency\n",
    "\n",
    "# One-dimensional features H0 and a scalar weight W for simplicity\n",
    "H0 = np.array([[1.0],\n",
    "               [2.0],\n",
    "               [3.0]])\n",
    "W = np.array([[1.5]])\n",
    "H1 = A_norm @ H0 @ W\n",
    "print(\"A_norm=\\n\", np.round(A_norm,3))\n",
    "print(\"H1=\\n\", np.round(H1,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fb8f8b",
   "metadata": {},
   "source": [
    "## 3) Build time windows → per-window graph samples\n",
    "\n",
    "We create **24→next** windows. Each training example is a graph with:\n",
    "- **x**: node features for the 3 cities (simple baselines start with *last 24 h mean & last value*).\n",
    "- **y**: next-hour **temperature** per node (3 values).\n",
    "\n",
    "Later we’ll add a tiny **temporal encoder** and more variables (wind) and switch to **GAT**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65d59de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6123 1293 1293 graph windows (train/val/test)\n"
     ]
    }
   ],
   "source": [
    "T = 24  # hours of history\n",
    "# Scale temps per city using train-only stats\n",
    "n = len(df)\n",
    "i_tr, i_va = int(0.70*n), int(0.85*n)\n",
    "sc_t2m = StandardScaler().fit(df[CITY_COLS_T2M].iloc[:i_tr])\n",
    "\n",
    "def make_graph_windows(frame: pd.DataFrame, with_wind=False):\n",
    "    X_graphs, Y_graphs, times = [], [], []\n",
    "    values_t2m = sc_t2m.transform(frame[CITY_COLS_T2M].values)  # (N,3) scaled\n",
    "    values_ws  = frame[CITY_COLS_WS].values if with_wind else None\n",
    "\n",
    "    for t in range(len(frame) - T - 1):\n",
    "        # Basic summary features per node for simplicity (keep it very simple for students)\n",
    "        hist_t2m = values_t2m[t:t+T]           # (T,3) scaled\n",
    "        last_t2m = values_t2m[t+T-1]           # (3,)\n",
    "        mean_t2m = hist_t2m.mean(axis=0)       # (3,)\n",
    "        std_t2m  = hist_t2m.std(axis=0)        # (3,)\n",
    "\n",
    "        # Start with [last, mean, std] as 3 features per node → (3 nodes, 3 feats)\n",
    "        x_feats = np.stack([last_t2m, mean_t2m, std_t2m], axis=1)  # (3,3)\n",
    "\n",
    "        # Optional: include wind (mean over window) as an extra feature column\n",
    "        if with_wind and values_ws is not None:\n",
    "            hist_ws = values_ws[t:t+T]         # (T,3) raw m/s\n",
    "            mean_ws = hist_ws.mean(axis=0)     # (3,)\n",
    "            x_feats = np.concatenate([x_feats, mean_ws[:,None]], axis=1)  # (3,4)\n",
    "\n",
    "        # Target: next-hour temps (scaled) for each city\n",
    "        y_next = values_t2m[t+T]               # (3,)\n",
    "        X_graphs.append(torch.tensor(x_feats, dtype=torch.float32))    # x\n",
    "        Y_graphs.append(torch.tensor(y_next,  dtype=torch.float32))    # y\n",
    "        times.append(frame['time'].iloc[t+T])\n",
    "    return X_graphs, Y_graphs, times\n",
    "\n",
    "# Build splits\n",
    "train_df = df.iloc[:i_tr].copy()\n",
    "val_df   = df.iloc[i_tr:i_va].copy()\n",
    "test_df  = df.iloc[i_va:].copy()\n",
    "\n",
    "Xtr, ytr, _ = make_graph_windows(train_df, with_wind=False)\n",
    "Xva, yva, _ = make_graph_windows(val_df,   with_wind=False)\n",
    "Xte, yte, _ = make_graph_windows(test_df,  with_wind=False)\n",
    "\n",
    "print(len(Xtr), len(Xva), len(Xte), \"graph windows (train/val/test)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5850014",
   "metadata": {},
   "source": [
    "## 4) PyG dataset wrappers (3-node graph per window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "285af0da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyG datasets ready (batch_size=1 for clarity)\n"
     ]
    }
   ],
   "source": [
    "class GraphWindowDataset(Dataset):\n",
    "    def __init__(self, X_list, y_list, edge_index):\n",
    "        self.X = X_list\n",
    "        self.y = y_list\n",
    "        self.edge_index = edge_index\n",
    "    def __len__(self): return len(self.X)\n",
    "    def __getitem__(self, i):\n",
    "        x = self.X[i]                 # (3, F)\n",
    "        y = self.y[i].unsqueeze(1)    # (3, 1) — predict each node's next-hour temp\n",
    "        data = Data(x=x, edge_index=edge_index)\n",
    "        data.y = y\n",
    "        return data\n",
    "\n",
    "if PYG_OK:\n",
    "    tr_ds = GraphWindowDataset(Xtr, ytr, edge_index)\n",
    "    va_ds = GraphWindowDataset(Xva, yva, edge_index)\n",
    "    te_ds = GraphWindowDataset(Xte, yte, edge_index)\n",
    "\n",
    "    def pyg_collate(batch):\n",
    "        # Batch small graphs by stacking nodes and offsetting edge indices automatically via PyG Batch? \n",
    "        # To keep this ultra-simple and transparent, we will process per-sample without PyG's Batch.\n",
    "        return batch\n",
    "\n",
    "    tr_dl = DataLoader(tr_ds, batch_size=1, shuffle=True, collate_fn=pyg_collate)\n",
    "    va_dl = DataLoader(va_ds, batch_size=1, shuffle=False, collate_fn=pyg_collate)\n",
    "    te_dl = DataLoader(te_ds, batch_size=1, shuffle=False, collate_fn=pyg_collate)\n",
    "\n",
    "    print(\"PyG datasets ready (batch_size=1 for clarity)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5298640e",
   "metadata": {},
   "source": [
    "## 5) Example 1 — **GCN**: last/mean/std → next-hour temps (per node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68999a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train MSE=0.0545 | val MSE=0.0372\n",
      "Epoch 02 | train MSE=0.0428 | val MSE=0.0333\n",
      "Epoch 03 | train MSE=0.0415 | val MSE=0.0371\n",
      "Epoch 04 | train MSE=0.0411 | val MSE=0.0336\n",
      "Epoch 05 | train MSE=0.0412 | val MSE=0.0329\n",
      "Epoch 06 | train MSE=0.0406 | val MSE=0.0346\n",
      "Epoch 07 | train MSE=0.0405 | val MSE=0.0354\n",
      "Epoch 08 | train MSE=0.0398 | val MSE=0.0336\n",
      "Epoch 09 | train MSE=0.0396 | val MSE=0.0336\n",
      "Epoch 10 | train MSE=0.0397 | val MSE=0.0338\n",
      "{'Dublin': 0.8208643198013306, 'Galway': 0.9079791307449341, 'Cork': 0.9440993070602417}\n"
     ]
    }
   ],
   "source": [
    "if not PYG_OK:\n",
    "    raise SystemExit(\"Install PyTorch Geometric to run this section.\")\n",
    "\n",
    "in_feats = Xtr[0].shape[1]   # 3\n",
    "hidden = 16\n",
    "\n",
    "class SimpleGCN(nn.Module):\n",
    "    def __init__(self, in_feats, hidden, out_dim=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_feats, hidden)\n",
    "        self.conv2 = GCNConv(hidden, hidden)\n",
    "        self.head  = nn.Linear(hidden, out_dim)  # per-node regressor\n",
    "    def forward(self, data: Data):\n",
    "        x, ei = data.x, data.edge_index\n",
    "        x = torch.relu(self.conv1(x, ei))\n",
    "        x = torch.relu(self.conv2(x, ei))\n",
    "        out = self.head(x)  # (3,1)\n",
    "        return out\n",
    "\n",
    "torch.manual_seed(0)\n",
    "model_gcn = SimpleGCN(in_feats, hidden).to(device)\n",
    "opt = torch.optim.Adam(model_gcn.parameters(), lr=3e-3)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "def loop_epoch(model, loader, train=True):\n",
    "    model.train(train)\n",
    "    tot, n = 0.0, 0\n",
    "    for batch in loader:\n",
    "        # We used batch_size=1; batch is [Data]\n",
    "        data = batch[0]\n",
    "        if train:\n",
    "            opt.zero_grad()\n",
    "        pred = model(data)\n",
    "        loss = loss_fn(pred, data.y)\n",
    "        if train:\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        tot += loss.item()\n",
    "        n += 1\n",
    "    return tot / max(n,1)\n",
    "\n",
    "for ep in range(10):\n",
    "    tr = loop_epoch(model_gcn, tr_dl, train=True)\n",
    "    va = loop_epoch(model_gcn, va_dl, train=False)\n",
    "    print(f\"Epoch {ep+1:02d} | train MSE={tr:.4f} | val MSE={va:.4f}\")\n",
    "\n",
    "# Report RMSE (°C) on test\n",
    "model_gcn.eval()\n",
    "preds, trues = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in te_dl:\n",
    "        data = batch[0]\n",
    "        p = model_gcn(data).squeeze(1).cpu().numpy()  # (3,)\n",
    "        t = data.y.squeeze(1).cpu().numpy()           # (3,)\n",
    "        preds.append(p); trues.append(t)\n",
    "preds = np.vstack(preds); trues = np.vstack(trues)      # (#samples, 3)\n",
    "\n",
    "# inverse-transform back to °C for each city separately\n",
    "# We scaled per-city jointly using sc_t2m, so we invert city-wise.\n",
    "inv_pred = sc_t2m.inverse_transform(preds)\n",
    "inv_true = sc_t2m.inverse_transform(trues)\n",
    "rmse_each = np.sqrt(((inv_pred - inv_true)**2).mean(axis=0))\n",
    "print({k: float(v) for k,v in zip(['Dublin','Galway','Cork'], rmse_each)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a950fa1",
   "metadata": {},
   "source": [
    "## 6) Example 2 — **Temporal encoder + GCN**\n",
    "\n",
    "Instead of simple [last/mean/std] features, use a tiny **1D Conv** to encode the **last 24 h sequence** of each city's temperature to a vector per node, then apply a **GCN**. This mirrors the slides’ idea of combining temporal and spatial learning in a simple way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80b02c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train MSE=0.0701 | val MSE=0.0274\n",
      "Epoch 02 | train MSE=0.0430 | val MSE=0.0286\n",
      "Epoch 03 | train MSE=0.0391 | val MSE=0.0283\n",
      "Epoch 04 | train MSE=0.0398 | val MSE=0.0251\n",
      "Epoch 05 | train MSE=0.0361 | val MSE=0.0255\n",
      "Epoch 06 | train MSE=0.0344 | val MSE=0.0331\n",
      "Epoch 07 | train MSE=0.0337 | val MSE=0.0258\n",
      "Epoch 08 | train MSE=0.0321 | val MSE=0.0269\n",
      "Epoch 09 | train MSE=0.0324 | val MSE=0.0308\n",
      "Epoch 10 | train MSE=0.0322 | val MSE=0.0343\n",
      "SeqEnc+GCN RMSE (°C): {'Dublin': 1.055409550666809, 'Galway': 1.0983647108078003, 'Cork': 1.3047443628311157}\n"
     ]
    }
   ],
   "source": [
    "# Build new feature windows using a tiny temporal encoder on raw sequences.\n",
    "# For teaching clarity, we keep it short and CPU-friendly.\n",
    "T = 24\n",
    "vals_t2m_all = sc_t2m.transform(df[CITY_COLS_T2M].values)  # (N,3) scaled\n",
    "\n",
    "def make_seq_tensors(frame):\n",
    "    base = sc_t2m.transform(frame[CITY_COLS_T2M].values)  # (N,3)\n",
    "    Xseq, Y, times = [], [], []\n",
    "    for t in range(len(frame) - T - 1):\n",
    "        # sequence per city: (T, 3) → we will encode per node\n",
    "        seq = base[t:t+T]          # (T,3)\n",
    "        target = base[t+T]         # (3,)\n",
    "        Xseq.append(torch.tensor(seq, dtype=torch.float32))  # store raw (T,3)\n",
    "        Y.append(torch.tensor(target, dtype=torch.float32))\n",
    "        times.append(frame['time'].iloc[t+T])\n",
    "    return Xseq, Y, times\n",
    "\n",
    "Xtr_seq, ytr_seq, _ = make_seq_tensors(train_df)\n",
    "Xva_seq, yva_seq, _ = make_seq_tensors(val_df)\n",
    "Xte_seq, yte_seq, _ = make_seq_tensors(test_df)\n",
    "\n",
    "class SeqEncGCNDataset(Dataset):\n",
    "    def __init__(self, Xseq, y, edge_index):\n",
    "        self.Xseq = Xseq\n",
    "        self.y = y\n",
    "        self.edge_index = edge_index\n",
    "    def __len__(self): return len(self.Xseq)\n",
    "    def __getitem__(self, i):\n",
    "        # Encode sequences per node inside the model (clear & didactic)\n",
    "        data = Data(edge_index=edge_index)\n",
    "        data.seq = self.Xseq[i]           # (T,3)\n",
    "        data.y   = self.y[i].unsqueeze(1) # (3,1)\n",
    "        return data\n",
    "\n",
    "if PYG_OK:\n",
    "    tr2_ds = SeqEncGCNDataset(Xtr_seq, ytr_seq, edge_index)\n",
    "    va2_ds = SeqEncGCNDataset(Xva_seq, yva_seq, edge_index)\n",
    "    te2_ds = SeqEncGCNDataset(Xte_seq, yte_seq, edge_index)\n",
    "    tr2_dl = DataLoader(tr2_ds, batch_size=1, shuffle=True, collate_fn=lambda b: b)\n",
    "    va2_dl = DataLoader(va2_ds, batch_size=1, shuffle=False, collate_fn=lambda b: b)\n",
    "    te2_dl = DataLoader(te2_ds, batch_size=1, shuffle=False, collate_fn=lambda b: b)\n",
    "\n",
    "class TinySeqEncoder(nn.Module):\n",
    "    \"\"\"Encode a (T,3) temperature sequence into (3,F) node features via shared 1D convs.\n",
    "    We treat each city's sequence as a separate channel and apply depthwise-ish conv by looping (for clarity).\n",
    "    \"\"\"\n",
    "    def __init__(self, T=24, out_feats=8):\n",
    "        super().__init__()\n",
    "        # A simple MLP over the T-length vector per city (clearer than conv for slides)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(T, 32), nn.ReLU(),\n",
    "            nn.Linear(32, out_feats)\n",
    "        )\n",
    "    def forward(self, seq):  # seq: (T,3) scaled\n",
    "        # Split by city: shape -> list of (T,)\n",
    "        outs = []\n",
    "        for c in range(3):\n",
    "            v = seq[:, c]                 # (T,)\n",
    "            z = self.mlp(v)               # (out_feats,)\n",
    "            outs.append(z)\n",
    "        x = torch.stack(outs, dim=0)      # (3, out_feats)\n",
    "        return x\n",
    "\n",
    "class SeqEncGCN(nn.Module):\n",
    "    def __init__(self, enc_feats=8, hidden=16, out_dim=1):\n",
    "        super().__init__()\n",
    "        self.enc = TinySeqEncoder(T=24, out_feats=enc_feats)\n",
    "        self.gcn1 = GCNConv(enc_feats, hidden)\n",
    "        self.gcn2 = GCNConv(hidden, hidden)\n",
    "        self.head = nn.Linear(hidden, out_dim)\n",
    "    def forward(self, data: Data):\n",
    "        # encode per-node features from sequences\n",
    "        x = self.enc(data.seq)                 # (3, F)\n",
    "        x = torch.relu(self.gcn1(x, data.edge_index))\n",
    "        x = torch.relu(self.gcn2(x, data.edge_index))\n",
    "        return self.head(x)                    # (3,1)\n",
    "\n",
    "if PYG_OK:\n",
    "    torch.manual_seed(0)\n",
    "    model_seqgcn = SeqEncGCN().to(device)\n",
    "    opt2 = torch.optim.Adam(model_seqgcn.parameters(), lr=3e-3)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    def run_epoch(model, loader, train=True):\n",
    "        model.train(train)\n",
    "        tot = 0.0; n=0\n",
    "        for batch in loader:\n",
    "            data = batch[0]\n",
    "            if train: opt2.zero_grad()\n",
    "            pred = model(data)\n",
    "            loss = loss_fn(pred, data.y)\n",
    "            if train: loss.backward(); opt2.step()\n",
    "            tot += loss.item(); n += 1\n",
    "        return tot/max(n,1)\n",
    "\n",
    "    for ep in range(10):\n",
    "        tr = run_epoch(model_seqgcn, tr2_dl, train=True)\n",
    "        va = run_epoch(model_seqgcn, va2_dl, train=False)\n",
    "        print(f\"Epoch {ep+1:02d} | train MSE={tr:.4f} | val MSE={va:.4f}\")\n",
    "\n",
    "    # Test (°C)\n",
    "    model_seqgcn.eval()\n",
    "    P, Tt = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in te2_dl:\n",
    "            data = batch[0]\n",
    "            p = model_seqgcn(data).squeeze(1).cpu().numpy()\n",
    "            t = data.y.squeeze(1).cpu().numpy()\n",
    "            P.append(p); Tt.append(t)\n",
    "    P = np.vstack(P); Tt = np.vstack(Tt)\n",
    "    invP = sc_t2m.inverse_transform(P); invT = sc_t2m.inverse_transform(Tt)\n",
    "    rmse_each = np.sqrt(((invP - invT)**2).mean(axis=0))\n",
    "    print(\"SeqEnc+GCN RMSE (°C):\", {k: float(v) for k,v in zip(['Dublin','Galway','Cork'], rmse_each)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adc3551",
   "metadata": {},
   "source": [
    "## 7) Final example — **GAT (Graph Attention)** with **all variables**\n",
    "\n",
    "Features per node include **temperature and wind** over the last 24 h (simple MLP encoder).  \n",
    "We use a **GATConv** with multiple heads to let the model learn which neighbours matter more at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a937b6ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train MSE=0.0910 | val MSE=0.0408\n",
      "Epoch 02 | train MSE=0.0478 | val MSE=0.0295\n",
      "Epoch 03 | train MSE=0.0412 | val MSE=0.0266\n",
      "Epoch 04 | train MSE=0.0382 | val MSE=0.0292\n",
      "Epoch 05 | train MSE=0.0355 | val MSE=0.0264\n",
      "Epoch 06 | train MSE=0.0341 | val MSE=0.0259\n",
      "Epoch 07 | train MSE=0.0325 | val MSE=0.0232\n",
      "Epoch 08 | train MSE=0.0323 | val MSE=0.0242\n",
      "Epoch 09 | train MSE=0.0298 | val MSE=0.0230\n",
      "Epoch 10 | train MSE=0.0300 | val MSE=0.0207\n",
      "Epoch 11 | train MSE=0.0296 | val MSE=0.0224\n",
      "Epoch 12 | train MSE=0.0287 | val MSE=0.0284\n",
      "GAT (all vars) RMSE (°C): {'Dublin': 0.7425894737243652, 'Galway': 0.9258968234062195, 'Cork': 0.8738669157028198}\n"
     ]
    }
   ],
   "source": [
    "# Build sequence windows that include both t2m (scaled) and wind (unscaled) per city.\n",
    "# We'll encode each city's 2-variable sequence with a tiny MLP (flatten over time for simplicity).\n",
    "T = 24\n",
    "sc_all = StandardScaler().fit(df[CITY_COLS_T2M].iloc[:i_tr])  # for temps; winds left raw to keep simple\n",
    "\n",
    "def make_multi_seq(frame):\n",
    "    t2m = sc_all.transform(frame[CITY_COLS_T2M].values)   # (N,3) scaled\n",
    "    ws  = frame[CITY_COLS_WS].values                      # (N,3) raw m/s\n",
    "    Xseq, Y = [], []\n",
    "    for t in range(len(frame)-T-1):\n",
    "        # build per-city sequence with 2 vars: (T, 2) per city\n",
    "        seq_city = []\n",
    "        for c in range(3):\n",
    "            seq = np.stack([t2m[t:t+T, c], ws[t:t+T, c]], axis=1)  # (T,2)\n",
    "            seq_city.append(seq.reshape(-1))                       # flatten to length 2T\n",
    "        x = np.stack(seq_city, axis=0)   # (3, 2T)\n",
    "        y = t2m[t+T]                     # next-hour target temps (scaled)\n",
    "        Xseq.append(torch.tensor(x, dtype=torch.float32))\n",
    "        Y.append(torch.tensor(y, dtype=torch.float32))\n",
    "    return Xseq, Y\n",
    "\n",
    "Xtr_m, ytr_m = make_multi_seq(train_df)\n",
    "Xva_m, yva_m = make_multi_seq(val_df)\n",
    "Xte_m, yte_m = make_multi_seq(test_df)\n",
    "\n",
    "class MultiEncGATDataset(Dataset):\n",
    "    def __init__(self, X, y, edge_index):\n",
    "        self.X = X; self.y = y; self.edge_index = edge_index\n",
    "    def __len__(self): return len(self.X)\n",
    "    def __getitem__(self, i):\n",
    "        d = Data(edge_index=edge_index)\n",
    "        d.x = self.X[i]                 # (3, 2T)\n",
    "        d.y = self.y[i].unsqueeze(1)    # (3,1)\n",
    "        return d\n",
    "\n",
    "if PYG_OK:\n",
    "    tr3_ds = MultiEncGATDataset(Xtr_m, ytr_m, edge_index)\n",
    "    va3_ds = MultiEncGATDataset(Xva_m, yva_m, edge_index)\n",
    "    te3_ds = MultiEncGATDataset(Xte_m, yte_m, edge_index)\n",
    "    tr3_dl = DataLoader(tr3_ds, batch_size=1, shuffle=True,  collate_fn=lambda b: b)\n",
    "    va3_dl = DataLoader(va3_ds, batch_size=1, shuffle=False, collate_fn=lambda b: b)\n",
    "    te3_dl = DataLoader(te3_ds, batch_size=1, shuffle=False, collate_fn=lambda b: b)\n",
    "\n",
    "class GATAllVars(nn.Module):\n",
    "    def __init__(self, in_feats, hidden=16, heads=2, out_dim=1):\n",
    "        super().__init__()\n",
    "        self.enc = nn.Sequential(             # simple MLP per node to compress 2T → hidden\n",
    "            nn.Linear(in_feats, 64), nn.ReLU(),\n",
    "            nn.Linear(64, hidden)\n",
    "        )\n",
    "        self.gat1 = GATConv(hidden, hidden, heads=heads, concat=True, dropout=0.0)\n",
    "        self.gat2 = GATConv(hidden*heads, hidden, heads=1, concat=False, dropout=0.0)\n",
    "        self.head = nn.Linear(hidden, out_dim)\n",
    "    def forward(self, data: Data):\n",
    "        x = self.enc(data.x)                          # (3, hidden)\n",
    "        x = torch.relu(self.gat1(x, data.edge_index))\n",
    "        x = torch.relu(self.gat2(x, data.edge_index))\n",
    "        return self.head(x)                           # (3,1)\n",
    "\n",
    "if PYG_OK:\n",
    "    torch.manual_seed(0)\n",
    "    model_gat = GATAllVars(in_feats=2*T).to(device)\n",
    "    opt3 = torch.optim.Adam(model_gat.parameters(), lr=3e-3)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    def run_gat_epoch(model, loader, train=True):\n",
    "        model.train(train); tot=0.0; n=0\n",
    "        for batch in loader:\n",
    "            data = batch[0]\n",
    "            if train: opt3.zero_grad()\n",
    "            pred = model(data)\n",
    "            loss = loss_fn(pred, data.y)\n",
    "            if train: loss.backward(); opt3.step()\n",
    "            tot += loss.item(); n += 1\n",
    "        return tot/max(n,1)\n",
    "\n",
    "    for ep in range(12):\n",
    "        tr = run_gat_epoch(model_gat, tr3_dl, train=True)\n",
    "        va = run_gat_epoch(model_gat, va3_dl, train=False)\n",
    "        print(f\"Epoch {ep+1:02d} | train MSE={tr:.4f} | val MSE={va:.4f}\")\n",
    "\n",
    "    # Test (°C)\n",
    "    model_gat.eval()\n",
    "    P, Tt = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in te3_dl:\n",
    "            data = batch[0]\n",
    "            p = model_gat(data).squeeze(1).cpu().numpy()\n",
    "            t = data.y.squeeze(1).cpu().numpy()\n",
    "            P.append(p); Tt.append(t)\n",
    "    P = np.vstack(P); Tt = np.vstack(Tt)\n",
    "    invP = sc_all.inverse_transform(P); invT = sc_all.inverse_transform(Tt)\n",
    "    rmse_each = np.sqrt(((invP - invT)**2).mean(axis=0))\n",
    "    print(\"GAT (all vars) RMSE (°C):\", {k: float(v) for k,v in zip(['Dublin','Galway','Cork'], rmse_each)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45176d9f",
   "metadata": {},
   "source": [
    "## 8) Notes & extensions\n",
    "\n",
    "- Try **fully connected** edges (add Dublin↔Cork) and compare.\n",
    "- Change window `T` (12/48) — see if GAT benefits more from longer context.\n",
    "- Swap the simple MLP temporal encoder for a **1D CNN** or **small LSTM** per node.\n",
    "- Add more stations as new nodes; update `edge_index` via distance-based k-NN.\n",
    "- Predict **multi-step** outputs (e.g., next 6 hours) by changing `y` and the head.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stat41130",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
