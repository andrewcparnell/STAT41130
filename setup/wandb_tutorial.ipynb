{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c8be5ba",
   "metadata": {},
   "source": [
    "\n",
    "# STAT41130 — wandb + PyTorch (No-Prompt Login)\n",
    "\n",
    "This notebook shows a tiny PyTorch model **with Weights & Biases tracking** — and it **won’t prompt** for an API key in VS Code notebooks.\n",
    "\n",
    "**Login flow used here**\n",
    "1. If `WANDB_API_KEY` env var is set → log in with it (online).\n",
    "2. Else if you paste an **inline key** into the cell below → log in with it (online).\n",
    "3. Otherwise → **auto-switch to offline mode** (no prompt), logs saved locally to sync later.\n",
    "\n",
    "**Quick start**\n",
    "- Online (recommended): set env vars in a `.env` file or your shell:\n",
    "  ```bash\n",
    "  export WANDB_API_KEY=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
    "  export WANDB_ENTITY=your-team-or-user\n",
    "  export WANDB_PROJECT=stat41130-demo\n",
    "  ```\n",
    "- Offline: do nothing; the cell below will set `WANDB_MODE=offline` automatically.\n",
    "- Optional one-time terminal login also works: `wandb login`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962c1ac9",
   "metadata": {},
   "source": [
    "\n",
    "## 0) Install W&B (one-time)\n",
    "If missing, install with pip.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87afc4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %pip install --quiet wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "220920c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, math, time\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    import wandb\n",
    "except ImportError as e:\n",
    "    raise SystemExit(\"wandb is not installed. Run `pip install wandb` and re-run this cell.\") from e\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b5bbb0",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Configure W&B (no interactive prompt)\n",
    "- **Preferred:** set `WANDB_API_KEY`, `WANDB_ENTITY`, `WANDB_PROJECT` in your environment.\n",
    "- **Inline key (temporary):** paste your key into `INLINE_KEY` **only on your machine** (don’t commit to Git).\n",
    "- If neither is provided, we **default to offline mode** to avoid prompts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4230d419",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\andre\\_netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mandrew-parnell\u001b[0m (\u001b[33mandrew-parnell-university-college-dublin\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[wandb] Logged in with provided API key.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\andre\\Documents\\GitHub\\STAT41130\\setup\\wandb\\run-20251005_181547-ocxhs34b</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/andrew-parnell-university-college-dublin/stat41130-demo/runs/ocxhs34b' target=\"_blank\">mlp-noprompt-1759684547</a></strong> to <a href='https://wandb.ai/andrew-parnell-university-college-dublin/stat41130-demo' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/andrew-parnell-university-college-dublin/stat41130-demo' target=\"_blank\">https://wandb.ai/andrew-parnell-university-college-dublin/stat41130-demo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/andrew-parnell-university-college-dublin/stat41130-demo/runs/ocxhs34b' target=\"_blank\">https://wandb.ai/andrew-parnell-university-college-dublin/stat41130-demo/runs/ocxhs34b</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W&B run: https://wandb.ai/andrew-parnell-university-college-dublin/stat41130-demo/runs/ocxhs34b\n"
     ]
    }
   ],
   "source": [
    "\n",
    "CFG = {\n",
    "    \"seed\": 42,\n",
    "    \"epochs\": 40,\n",
    "    \"batch_size\": 64,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"hidden\": 16,\n",
    "    \"dataset\": \"synthetic-rings\",\n",
    "    \"architecture\": \"MLP(2->16->1)\",\n",
    "}\n",
    "\n",
    "# Inline key (OPTIONAL). Leave empty to avoid hardcoding secrets.\n",
    "INLINE_KEY = \"\"  # e.g., \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n",
    "\n",
    "# Pick up env vars; you can set these in a .env file or your shell.\n",
    "ENTITY  = os.getenv(\"WANDB_ENTITY\", \"your-wandb-entity\")   # change if desired\n",
    "PROJECT = os.getenv(\"WANDB_PROJECT\", \"stat41130-demo\")     # change if desired\n",
    "\n",
    "# Decide login method without prompting.\n",
    "api_key = os.getenv(\"WANDB_API_KEY\") or INLINE_KEY\n",
    "if api_key:\n",
    "    try:\n",
    "        wandb.login(key=api_key)\n",
    "        print(\"[wandb] Logged in with provided API key.\")\n",
    "    except Exception as e:\n",
    "        print(f\"[wandb] Login failed: {e} — switching to offline mode.\")\n",
    "        os.environ[\"WANDB_MODE\"] = \"offline\"\n",
    "else:\n",
    "    # No key provided → offline by default to avoid interactive prompt in VS Code\n",
    "    if os.getenv(\"WANDB_MODE\", \"\").lower() != \"offline\":\n",
    "        os.environ[\"WANDB_MODE\"] = \"offline\"\n",
    "    print(\"[wandb] No API key found — using OFFLINE mode (set WANDB_API_KEY to enable online logging).\")\n",
    "\n",
    "run = wandb.init(\n",
    "    entity=ENTITY,\n",
    "    project=PROJECT,\n",
    "    config=CFG,\n",
    "    name=f\"mlp-noprompt-{int(time.time())}\",\n",
    "    notes=\"STAT41130 minimal PyTorch + wandb (no prompt)\",\n",
    ")\n",
    "print(\"W&B run:\", (run.url if getattr(run, \"url\", None) else \"offline/no-url\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b005c96",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Make a tiny synthetic dataset\n",
    "2D inputs, two classes with slightly different radii (quick to train).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c020addb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1000, 2]),\n",
       " torch.Size([1000, 1]),\n",
       " torch.Size([300, 2]),\n",
       " torch.Size([300, 1]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def make_tiny_dataset(n_train=1000, n_val=300, seed=0):\n",
    "    g = torch.Generator().manual_seed(seed)\n",
    "    def sample(n):\n",
    "        theta = torch.rand(n, generator=g) * 2*math.pi\n",
    "        y = (torch.rand(n, generator=g) > 0.5).float()\n",
    "        r = (1.0 + y*0.6) + 0.15*torch.randn(n, generator=g)\n",
    "        x1 = r * torch.cos(theta)\n",
    "        x2 = r * torch.sin(theta)\n",
    "        X = torch.stack([x1, x2], dim=1)\n",
    "        return X, y.unsqueeze(1)\n",
    "    Xtr, ytr = sample(n_train)\n",
    "    Xva, yva = sample(n_val)\n",
    "    return (Xtr, ytr), (Xva, yva)\n",
    "\n",
    "(Xtr, ytr), (Xva, yva) = make_tiny_dataset(seed=CFG[\"seed\"])\n",
    "train_loader = DataLoader(TensorDataset(Xtr, ytr), batch_size=CFG[\"batch_size\"], shuffle=True)\n",
    "val_loader   = DataLoader(TensorDataset(Xva, yva), batch_size=CFG[\"batch_size\"], shuffle=False)\n",
    "\n",
    "Xtr.shape, ytr.shape, Xva.shape, yva.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94431cee",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Define a tiny MLP classifier\n",
    "One hidden layer with ReLU; **BCEWithLogitsLoss** for binary classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d646d1f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TinyMLP(\n",
       "  (fc1): Linear(in_features=2, out_features=16, bias=True)\n",
       "  (fc2): Linear(in_features=16, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class TinyMLP(nn.Module):\n",
    "    def __init__(self, hidden=16):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(2, hidden)\n",
    "        self.fc2 = nn.Linear(hidden, 1)\n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.fc1(x))\n",
    "        return self.fc2(h)  # logits\n",
    "\n",
    "model = TinyMLP(hidden=CFG[\"hidden\"])\n",
    "opt = torch.optim.Adam(model.parameters(), lr=CFG[\"learning_rate\"])\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Track gradients/parameters in W&B (harmless in offline too)\n",
    "wandb.watch(model, criterion=criterion, log=\"all\", log_freq=10)\n",
    "model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d321f59a",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Train and log metrics\n",
    "Logs: **loss**, **accuracy**, and a simple **decision boundary plot** (every 10 epochs).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9dbef5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | train_acc=0.712 val_acc=0.673\n",
      "Epoch 010 | train_acc=0.973 val_acc=0.983\n",
      "Epoch 020 | train_acc=0.973 val_acc=0.980\n",
      "Epoch 030 | train_acc=0.976 val_acc=0.983\n",
      "Epoch 040 | train_acc=0.972 val_acc=0.980\n",
      "Training complete. Best val acc: 0.9833333365122477\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def accuracy_from_logits(logits, y):\n",
    "    probs = torch.sigmoid(logits)\n",
    "    preds = (probs >= 0.5).float()\n",
    "    return (preds.eq(y).float().mean().item())\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    total_loss, total_acc, n = 0.0, 0.0, 0\n",
    "    for xb, yb in loader:\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        acc = accuracy_from_logits(logits, yb)\n",
    "        bsz = xb.size(0)\n",
    "        total_loss += loss.item() * bsz\n",
    "        total_acc  += acc * bsz\n",
    "        n += bsz\n",
    "    return total_loss / n, total_acc / n\n",
    "\n",
    "def plot_decision_boundary(model, X, y, step=0.05):\n",
    "    # decision surface\n",
    "    x_min, x_max = X[:,0].min().item()-0.5, X[:,0].max().item()+0.5\n",
    "    y_min, y_max = X[:,1].min().item()-0.5, X[:,1].max().item()+0.5\n",
    "    xs = torch.arange(x_min, x_max, step)\n",
    "    ys = torch.arange(y_min, y_max, step)\n",
    "    xx, yy = torch.meshgrid(xs, ys, indexing='ij')\n",
    "    grid = torch.stack([xx.reshape(-1), yy.reshape(-1)], dim=1)\n",
    "    with torch.no_grad():\n",
    "        zz = torch.sigmoid(model(grid)).reshape_as(xx)\n",
    "    fig, ax = plt.subplots(figsize=(4,4))\n",
    "    ax.contourf(xx, yy, zz, levels=20, alpha=0.6)  # no explicit colors\n",
    "    # overlay points without specifying colors; use different markers\n",
    "    y_flat = y.squeeze()\n",
    "    mask0 = (y_flat == 0)\n",
    "    mask1 = ~mask0\n",
    "    ax.scatter(X[mask0,0], X[mask0,1], s=10, marker='o', edgecolor='k', label='class 0')\n",
    "    ax.scatter(X[mask1,0], X[mask1,1], s=10, marker='x', label='class 1')\n",
    "    ax.legend(loc=\"best\")\n",
    "    ax.set_title(\"Decision boundary (prob class=1)\")\n",
    "    ax.set_xlabel(\"x1\"); ax.set_ylabel(\"x2\")\n",
    "    fig.tight_layout()\n",
    "    return fig\n",
    "\n",
    "best_val_acc = 0.0\n",
    "for epoch in range(1, CFG[\"epochs\"] + 1):\n",
    "    model.train()\n",
    "    for xb, yb in train_loader:\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "    tr_loss, tr_acc = evaluate(model, train_loader)\n",
    "    va_loss, va_acc = evaluate(model, val_loader)\n",
    "\n",
    "    log_dict = {\"epoch\": epoch, \"train/loss\": tr_loss, \"train/acc\": tr_acc,\n",
    "                \"val/loss\": va_loss, \"val/acc\": va_acc}\n",
    "    if epoch % 10 == 0 or epoch == 1:\n",
    "        fig = plot_decision_boundary(model, Xtr, ytr)\n",
    "        log_dict[\"plots/decision_boundary\"] = wandb.Image(fig)\n",
    "        plt.close(fig)\n",
    "\n",
    "    wandb.log(log_dict)\n",
    "\n",
    "    if va_acc > best_val_acc:\n",
    "        best_val_acc = va_acc\n",
    "        path = \"best_model.pt\"\n",
    "        torch.save({\"state_dict\": model.state_dict(), \"cfg\": CFG}, path)\n",
    "        artifact = wandb.Artifact(\"tiny_mlp_best\", type=\"model\")\n",
    "        artifact.add_file(path)\n",
    "        wandb.log_artifact(artifact)\n",
    "\n",
    "    if epoch % 10 == 0 or epoch == 1:\n",
    "        print(f\"Epoch {epoch:03d} | train_acc={tr_acc:.3f} val_acc={va_acc:.3f}\")\n",
    "\n",
    "print(\"Training complete. Best val acc:\", best_val_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2db48577",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/acc</td><td>▁▆▇███▇███▇██████▇▇████████████▇█▇██████</td></tr><tr><td>train/loss</td><td>█▆▄▃▂▂▂▂▁▁▂▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▂▁▂▁▁▁▁▁▁</td></tr><tr><td>val/acc</td><td>▁▅▇▇▇█▇███▇███████▇████████████▇████████</td></tr><tr><td>val/loss</td><td>█▆▄▃▂▂▂▁▁▁▂▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▂▁▁▁▁▁▁▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>40</td></tr><tr><td>train/acc</td><td>0.972</td></tr><tr><td>train/loss</td><td>0.07444</td></tr><tr><td>val/acc</td><td>0.98</td></tr><tr><td>val/loss</td><td>0.05882</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">mlp-noprompt-1759684547</strong> at: <a href='https://wandb.ai/andrew-parnell-university-college-dublin/stat41130-demo/runs/ocxhs34b' target=\"_blank\">https://wandb.ai/andrew-parnell-university-college-dublin/stat41130-demo/runs/ocxhs34b</a><br> View project at: <a href='https://wandb.ai/andrew-parnell-university-college-dublin/stat41130-demo' target=\"_blank\">https://wandb.ai/andrew-parnell-university-college-dublin/stat41130-demo</a><br>Synced 5 W&B file(s), 5 media file(s), 12 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251005_181547-ocxhs34b\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Finish the W&B run (uploads if online; stores locally if offline).\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ede318",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Syncing offline runs later\n",
    "If you ran in offline mode, W&B saved runs locally (e.g., `wandb/offline-run-*`). To sync later from a terminal:\n",
    "```bash\n",
    "wandb login  # if you haven't already\n",
    "wandb sync wandb/offline-run-*\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stat41130-py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
